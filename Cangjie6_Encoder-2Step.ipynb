{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from fontTools.ttLib import TTFont\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "VOCAB = 28; EBD_DIM = 256; UNIT_DIM = 256; BATCH_SIZE_PER_REPLICA = 128\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "REPLICA_SIZE = strategy.num_replicas_in_sync; BATCH_SIZE = BATCH_SIZE_PER_REPLICA * REPLICA_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glyph(object):\n",
    "    # transform character to bitmap\n",
    "    def __init__(self, fonts, size=64):\n",
    "        # load fonts, size. We will use 2 fonts for all CJK characters, so keep 2 codepoint books.\n",
    "        self.codepoints = [set() for _ in fonts]\n",
    "        self.size = int(size * 0.9)\n",
    "        self.size_img = size\n",
    "        self.pad = (size - self.size) // 2\n",
    "        self.fonts = [ImageFont.truetype(f, self.size) for f in fonts]\n",
    "        # use a cache to reduce computation if duplicated characters encountered.\n",
    "        self.cache = {}\n",
    "        for cp, font in zip(self.codepoints, fonts):\n",
    "            font = TTFont(font)\n",
    "            # store codepoints in font cmap into self.codepoints\n",
    "            for cmap in font['cmap'].tables:\n",
    "                if not cmap.isUnicode():\n",
    "                    continue\n",
    "                for k in cmap.cmap:\n",
    "                    cp.add(k)\n",
    "    \n",
    "    def draw(self, ch):\n",
    "        if ch in self.cache:\n",
    "            return self.cache[ch]\n",
    "        # search among fonts, use the first found\n",
    "        exist = False\n",
    "        for i in range(len(self.codepoints)):\n",
    "            if ord(ch) in self.codepoints[i]:\n",
    "                font = self.fonts[i]\n",
    "                exist = True\n",
    "                break\n",
    "        if not exist:\n",
    "            return None\n",
    "\n",
    "        img = Image.new('L', (self.size_img, self.size_img), 0)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        (width, baseline), (offset_x, offset_y) = font.font.getsize(ch)\n",
    "        draw.text((self.pad - offset_x, self.pad - offset_y + 4), ch, font=font, fill=255, stroke_fill=255) \n",
    "        img_array = np.array(img.getdata(), dtype='float32').reshape((self.size_img, self.size_img)) / 255\n",
    "        self.cache[ch] = img_array\n",
    "\n",
    "        return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphbook = Glyph(['data/fonts/TH-Ming-HP0.ttf', 'data/fonts/TH-Ming-P2.ttf'], size=64)\n",
    "\n",
    "def _mapping(item):\n",
    "    char, code, dup_total, dup_curr = item\n",
    "    glyph = glyphbook.draw(char)\n",
    "    if glyph is not None:\n",
    "        return glyph, code, dup_total, dup_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chart(chart, cores=multiprocessing.cpu_count()):\n",
    "    glyphs = []; codes = []\n",
    "    dup_total = []; dup_curr = []\n",
    "    if cores > 0:\n",
    "        with multiprocessing.Pool(processes=cores) as pool:\n",
    "            for item in pool.map(_mapping, chart.values):\n",
    "                if item is not None:\n",
    "                    glyphs.append(item[0])\n",
    "                    codes.append(item[1])\n",
    "                    dup_total.append(item[2])\n",
    "                    dup_curr.append(item[3])\n",
    "    else:\n",
    "        for item in np.apply_along_axis(_mapping, axis = 1, arr = chart.values):\n",
    "            if item is not None:\n",
    "                glyphs.append(item[0])\n",
    "                codes.append(item[1])\n",
    "                dup_total.append(item[2])\n",
    "                dup_curr.append(item[3])\n",
    "    return np.expand_dims(np.array(glyphs), -1), np.array(codes), np.array(dup_total), np.array(dup_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(code_table):\n",
    "    # Cangjie code consists only of a-z, with maximum length of 5, minimum of 1\n",
    "    # start with 0, a-z are 1-26, end and padding are 27\n",
    "    tokens = np.zeros((*code_table.shape, 1), dtype='int64')\n",
    "    code_index = list(map(lambda x: list(map(lambda y: ord(y) - 96, list(x))) + [27] * (5-len(x)), code_table))\n",
    "    tokens = np.append(tokens, np.array(code_index), axis=-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chart = pd.read_csv('data/cangjie6.txt', delimiter='\\t', header=None, names=['Char', 'Code'], keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "for char, code in code_chart.values:\n",
    "    if char in count:\n",
    "        count[char].append(code)\n",
    "        count[char].sort(key=len)\n",
    "        count[char].sort(key=lambda x: (len(x), x))\n",
    "    else:\n",
    "        count[char] = [code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = code_chart.Code.map(len).max()\n",
    "MAX_DUP = max(map(lambda x: len(x), count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chart['DuplicateTotal'] = code_chart['Char'].map(count).map(len).copy()\n",
    "code_chart['DuplicateCurrent'] = code_chart.apply(lambda x: count[x['Char']].index(x['Code']) + 1, axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphs, codes, dups_total, dups_curr = preprocess_chart(code_chart)\n",
    "tokens = tokenizer(codes)\n",
    "lengths = np.array([len(list(filter(lambda i: i < VOCAB - 1 and i > 0, x))) for x in tokens])\n",
    "lengths = np.array([np.identity(MAX_LEN)[i-1] for i in lengths], dtype='int64')\n",
    "dups_total = np.array([np.identity(MAX_DUP)[i-1] for i in dups_total], dtype='int64')\n",
    "dups_curr = np.array([np.identity(MAX_DUP)[i-1] for i in dups_curr], dtype='int64')\n",
    "del code_chart, codes, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_glyphs, validation_glyphs,\n",
    " train_tokens, validation_tokens,\n",
    " train_lengths, validation_lengths,\n",
    " train_dups_total, validation_dups_total,\n",
    " train_dups_curr, validation_dups_curr) = train_test_split(\n",
    "    glyphs, tokens, lengths, dups_total, dups_curr, test_size=0.1, random_state=902)\n",
    "del glyphs, tokens, lengths, dups_total, dups_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = train_glyphs.shape[0]\n",
    "num_samples_val = validation_glyphs.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_glyphs, train_tokens, train_lengths, train_dups_total, train_dups_curr))\n",
    "dataset = dataset.shuffle(train_glyphs.shape[0]).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((validation_glyphs, validation_tokens, validation_lengths, validation_dups_total, validation_dups_curr))\n",
    "val_dataset = val_dataset.shuffle(validation_glyphs.shape[0]).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n",
    "\n",
    "del train_glyphs, validation_glyphs, train_tokens, validation_tokens, train_lengths, validation_lengths\n",
    "del train_dups_total, validation_dups_total, train_dups_curr, validation_dups_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "$$\\mathrm{Smooth\\ ReLU}(x;\\alpha):=\\frac{1}{2}\\left(\\log\\left(e^{2\\alpha x}+e^{2x}\\right)-\\log 2\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smooth_ReLU(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 alpha_initializer='zeros',\n",
    "                 alpha_regularizer=None,\n",
    "                 alpha_constraint=None,\n",
    "                 shared_axes=None,\n",
    "                 **kwargs):\n",
    "        super(Smooth_ReLU, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.alpha_initializer = tf.keras.initializers.get(alpha_initializer)\n",
    "        self.alpha_regularizer = tf.keras.regularizers.get(alpha_regularizer)\n",
    "        self.alpha_constraint = tf.keras.constraints.get(alpha_constraint)\n",
    "        if shared_axes is None:\n",
    "            self.shared_axes = None\n",
    "        elif not isinstance(shared_axes, (list, tuple)):\n",
    "            self.shared_axes = [shared_axes]\n",
    "        else:\n",
    "            self.shared_axes = list(shared_axes)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        param_shape = list(input_shape[1:])\n",
    "        if self.shared_axes is not None:\n",
    "            for i in self.shared_axes:\n",
    "                param_shape[i - 1] = 1\n",
    "        self.alpha = self.add_weight(\n",
    "            shape=param_shape,\n",
    "            name='alpha',\n",
    "            initializer=self.alpha_initializer,\n",
    "            regularizer=self.alpha_regularizer,\n",
    "            constraint=self.alpha_constraint)\n",
    "        # Set input spec\n",
    "        axes = {}\n",
    "        if self.shared_axes:\n",
    "            for i in range(1, len(input_shape)):\n",
    "                if i not in self.shared_axes:\n",
    "                    axes[i] = input_shape[i]\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape), axes=axes)\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = 2 * inputs\n",
    "        return 0.5 * (tf.math.reduce_logsumexp([self.alpha * inputs, inputs], axis = 0) - tf.math.log(2.0))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'alpha_initializer': tf.keras.initializers.serialize(self.alpha_initializer),\n",
    "            'alpha_regularizer': tf.keras.regularizers.serialize(self.alpha_regularizer),\n",
    "            'alpha_constraint': tf.keras.constraints.serialize(self.alpha_constraint),\n",
    "            'shared_axes': self.shared_axes\n",
    "        }\n",
    "        base_config = super(Smooth_ReLU, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_CNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, feature_dim, kernel_size, **kwargs):\n",
    "        super(Res_CNN, self).__init__(**kwargs)\n",
    "        self.feature_dim = feature_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x_identity = tf.identity(x)\n",
    "        x = self.cnn2(x)\n",
    "        x_identity2 = tf.identity(x)\n",
    "        x = self.cnn3(x + x_identity)\n",
    "        x = self.norm(x + x_identity2)\n",
    "        x = self.srelu(x)\n",
    "        return x\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape))\n",
    "        self.cnn1 = tf.keras.layers.Convolution2D(self.feature_dim, self.kernel_size, padding='same')\n",
    "        self.cnn2 = tf.keras.layers.Convolution2D(self.feature_dim, self.kernel_size, padding='same')\n",
    "        self.cnn3 = tf.keras.layers.Convolution2D(self.feature_dim, self.kernel_size, padding='same')\n",
    "        self.norm = tf.keras.layers.BatchNormalization()\n",
    "        self.srelu = Smooth_ReLU(alpha_initializer = 'lecun_normal', alpha_constraint = tf.keras.constraints.max_norm(max_value=0.5, axis=[]), shared_axes = [1, 2])\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'feature_dim': self.feature_dim,\n",
    "            'kernel_size': self.kernel_size\n",
    "        }\n",
    "        base_config = super(Res_CNN, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + [self.feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.layers.Layer):\n",
    "    # This is essentially a CNN layer, \n",
    "    def __init__(self, embedding_dim, **kwargs):\n",
    "        super(CNN_Encoder, self).__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "\n",
    "        # x shape after cnn1 == (batch_size, 64, 64, embedding_dim // 16)\n",
    "        x = self.res_cnn1(x)\n",
    "        # x shape after pool1 == (batch_size, 32, 32, embedding_dim // 16)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # x shape after cnn2 == (batch_size, 32, 32, embedding_dim // 4)\n",
    "        x = self.res_cnn2(x)\n",
    "        # x shape after pool2 == (batch_size, 16, 16, embedding_dim // 4)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # x shape after cnn3 == (batch_size, 16, 16, embedding_dim)\n",
    "        x = self.res_cnn3(x)\n",
    "\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.5)\n",
    "        return x\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = tf.keras.layers.InputSpec(ndim=len(input_shape))\n",
    "        self.res_cnn1 = Res_CNN(self.embedding_dim // 16, (3, 3))\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.res_cnn2 = Res_CNN(self.embedding_dim // 4, (3, 3))\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.res_cnn3 = Res_CNN(self.embedding_dim, (3, 3))\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "        }\n",
    "        base_config = super(CNN_Encoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-3] + [input_shape[-3] // 4, input_shape[-2] // 4, self.embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bahdanau_Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, attention_dim, **kwargs):\n",
    "        super(Bahdanau_Attention, self).__init__(**kwargs)\n",
    "        self.attention_dim = attention_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features, hidden = inputs\n",
    "        # features(CNN_Encoder output) shape == (batch_size, 256, embedding_dim)\n",
    "        features = tf.reshape(features, [tf.shape(features)[0], -1, features.shape[-1]])\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 1024, attention_dim)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 256, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W1 = tf.keras.layers.Dense(self.attention_dim)\n",
    "        self.W2 = tf.keras.layers.Dense(self.attention_dim)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'attention_dim': self.attention_dim,\n",
    "        }\n",
    "        base_config = super(Bahdanau_Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], input_shape.shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, max_length, hidden_size, vocab_size, **kwargs):\n",
    "        super(Simple_Decoder, self).__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        features, position = inputs\n",
    "        # y shape (batch_size, hidden_size)\n",
    "        y = self.embedding(position)\n",
    "        # x shape (batch_size, embedding_dim)\n",
    "        x, w = self.attention([features, y])\n",
    "        # x shape (batch_size, hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.srelu(x)\n",
    "        # x shape (batch_size, vocab_size)\n",
    "        x = self.fc2(x)\n",
    "        return x, w\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.attention = Bahdanau_Attention(self.embedding_dim)\n",
    "        self.fc1 = tf.keras.layers.Dense(self.hidden_size)\n",
    "        self.fc2 = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self.srelu = Smooth_ReLU(alpha_initializer = 'lecun_normal', alpha_constraint = tf.keras.constraints.max_norm(max_value=0.5, axis=[]))\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'max_length': self.max_length,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'vocab_size': self.vocab_size,\n",
    "        }\n",
    "        base_config = super(Simple_Decoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], self.vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, **kwargs):\n",
    "        super(Dense_Decoder, self).__init__(**kwargs)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x, d_t, d_c = inputs\n",
    "        # shape after pool == (batch_size, 8, 8, embedding_dim)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.srelu1(x)\n",
    "        x = tf.reshape(x, [tf.shape(x)[0], tf.math.reduce_prod(x.shape[1:])])\n",
    "        if d_t != None and d_c != None:\n",
    "            d = tf.concat([tf.cast(d_t, 'float32'), tf.cast(d_c, 'float32')], axis=-1)\n",
    "            x = tf.concat([d, x], axis=-1)\n",
    "        x = self.fc2(x)\n",
    "        x = self.srelu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.srelu3(x)\n",
    "        x = self.fc4(x)\n",
    "        # shape = (batch_size, max_length)\n",
    "        return x\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.fc1 = tf.keras.layers.Dense(self.max_length * 16)\n",
    "        self.fc2 = tf.keras.layers.Dense(self.max_length * 16)\n",
    "        self.fc3 = tf.keras.layers.Dense(self.max_length * 4)\n",
    "        self.fc4 = tf.keras.layers.Dense(self.max_length)\n",
    "        self.srelu1 = Smooth_ReLU(alpha_initializer = 'lecun_normal', alpha_constraint = tf.keras.constraints.max_norm(max_value=0.5, axis=[]), shared_axes = [1, 2])\n",
    "        self.srelu2 = Smooth_ReLU(alpha_initializer = 'lecun_normal', alpha_constraint = tf.keras.constraints.max_norm(max_value=0.5, axis=[]))\n",
    "        self.srelu3 = Smooth_ReLU(alpha_initializer = 'lecun_normal', alpha_constraint = tf.keras.constraints.max_norm(max_value=0.5, axis=[]))\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'max_length': self.max_length,\n",
    "        }\n",
    "        base_config = super(Dense_Decoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], self.max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size, max_length, **kwargs):\n",
    "        super(RNN_Decoder, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def call(self, inputs, training=True, teacher_forcing=False):\n",
    "        x, l, d_t, d_c, features, hidden = inputs\n",
    "        # x is forward direction, y is beckward direction\n",
    "        # defining attention as a separate model\n",
    "        l = tf.cast(l, 'float32')\n",
    "        hidden_0_with_length = tf.concat([l, hidden[0]], axis=-1)\n",
    "        context_vector, attention_weights = self.attention([features, hidden_0_with_length])\n",
    "        l = tf.expand_dims(l, 1)\n",
    "        d = tf.expand_dims(tf.concat([tf.cast(d_t, 'float32'), tf.cast(d_c, 'float32')], axis=-1), 1)\n",
    "\n",
    "        # x shape before is (batch_size, 1) since it is passed through one by one at a time\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        if teacher_forcing:\n",
    "            x = self.embedding(x)\n",
    "        else:\n",
    "            if not self.embedding.built:\n",
    "                self.embedding(x)\n",
    "            x = tf.tensordot(x, self.embedding.weights[0], axes=[-1,0])\n",
    "        # context_vector shape is (batch_size, embedding_dim)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # x shape is (batch_size, 1, hidden_size)\n",
    "        # state is new hidden used in next step\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.3)\n",
    "        x, state1 = self.gru1(x, initial_state = hidden[0], training=training)\n",
    "        x_identity = tf.identity(x)\n",
    "        x = tf.concat([d, l, x], axis=-1)\n",
    "        x, state2 = self.gru2(x, initial_state = hidden[1], training=training)\n",
    "        x_identity2 = tf.identity(x)\n",
    "        \n",
    "        x = x + x_identity\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.3)\n",
    "        x, state3 = self.gru3(x, initial_state = hidden[2], training=training)\n",
    "        # x shape (batch_size, 1, max_length + hidden_size)\n",
    "        x = tf.concat([d, l, x + x_identity2], axis=-1)\n",
    "        x = tf.reshape(x, [tf.shape(x)[0], tf.math.reduce_prod(x.shape[1:])])\n",
    "        # x shape (batch_size, hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.srelu(x)\n",
    "        # x shape (batch_size, vocab_size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, [state1, state2, state3], attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        # generate new hidden layer with different batch size\n",
    "        return [tf.zeros([batch_size, self.hidden_size]) for _ in range(3)]\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru1 = tf.keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.gru2 = tf.keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.gru3 = tf.keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.hidden_size)\n",
    "        self.fc2 = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self.srelu = Smooth_ReLU(alpha_initializer = 'lecun_normal', alpha_constraint = tf.keras.constraints.max_norm(max_value=0.5, axis=[]))\n",
    "        self.attention = Bahdanau_Attention(self.hidden_size)\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_length': self.max_length,\n",
    "        }\n",
    "        base_config = super(RNN_Decoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], self.vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cangjie(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, length, duplicate, **kwargs):\n",
    "        super(Cangjie, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.length = length\n",
    "        self.duplicate = duplicate\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 64, 64, 1], dtype=tf.float32)])\n",
    "    def encode(self, glyph):\n",
    "        return self.encoder(glyph, training=False)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 16, 16, EBD_DIM], dtype=tf.float32)])\n",
    "    def predict_duplicates(self, features):\n",
    "        return tf.nn.softmax(self.duplicate([features, None, None]), axis=-1)\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 16, 16, EBD_DIM], dtype=tf.float32),\n",
    "                                 tf.TensorSpec(shape=[None, MAX_DUP], dtype=tf.float32),\n",
    "                                 tf.TensorSpec(shape=[None, MAX_DUP], dtype=tf.int64)])\n",
    "    def predict_length(self, features, total_dups, curr_dups):\n",
    "        return tf.nn.softmax(self.length([features, total_dups, curr_dups]), axis=-1)\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 16, 16, EBD_DIM], dtype=tf.float32),\n",
    "                                  tf.TensorSpec(shape=[None, MAX_LEN], dtype=tf.float32),\n",
    "                                  tf.TensorSpec(shape=[None, MAX_DUP], dtype=tf.float32),\n",
    "                                  tf.TensorSpec(shape=[None, MAX_DUP], dtype=tf.int64)])\n",
    "    def decode(self, features, length, total_dups, curr_dups):\n",
    "        # start with 0\n",
    "        dec_input = tf.repeat(tf.constant([[[1] + [0] * (VOCAB - 1)]], dtype='float32'), tf.shape(features)[0], axis=0)\n",
    "        hidden = self.decoder.reset_state(batch_size=tf.shape(features)[0])\n",
    "        probability = tf.ones([tf.shape(features)[0]], dtype='float32')\n",
    "        # iterate predictions, no teacher forcing here\n",
    "        for i in range(MAX_LEN):\n",
    "            prediction, hidden, attention_weights = self.decoder([tf.expand_dims(dec_input[:, i, :], 1), length, total_dups, curr_dups, features, hidden], training=False, teacher_forcing=False)\n",
    "            # we need deterministic result\n",
    "            prediction = tf.math.softmax(prediction, axis=-1)\n",
    "            probability *= tf.math.reduce_max(prediction, axis=-1)\n",
    "            dec_input = tf.concat([dec_input, tf.expand_dims(prediction, 1)], axis=1)\n",
    "        return tf.math.argmax(dec_input, axis=-1), probability   \n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 64, 64, 1], dtype=tf.float32)])\n",
    "    def call(self, glyph):\n",
    "        features = self.encode(glyph)\n",
    "        total_dups = self.predict_duplicates(features)\n",
    "        dups_dict = tf.math.argmax(total_dups, axis=-1)\n",
    "        max_dup = tf.math.reduce_max(dups_dict) + 1\n",
    "\n",
    "        results = tf.zeros([tf.shape(glyph, out_type=tf.int64)[0], max_dup, MAX_LEN + 1], dtype='int64')\n",
    "        probs = tf.zeros([tf.shape(glyph, out_type=tf.int64)[0], max_dup], dtype='float32')\n",
    "        identity_matrix = tf.convert_to_tensor(np.identity(MAX_DUP), dtype='int64')\n",
    "\n",
    "        for i in range(max_dup):\n",
    "            curr_dups = tf.math.minimum(tf.math.argmax(total_dups, axis=-1), i)\n",
    "            curr_dups = tf.nn.embedding_lookup(identity_matrix, curr_dups)\n",
    "            length = self.predict_length(features, total_dups, curr_dups)\n",
    "            test_result, prob = self.decode(features, length, total_dups, curr_dups)\n",
    "            results = tf.concat([results[:, :i, :], tf.expand_dims(test_result, axis=1), tf.zeros([tf.shape(glyph, out_type=tf.int64)[0], max_dup - i - 1, MAX_LEN + 1], dtype='int64')], axis=1)\n",
    "            probs = tf.concat([probs[:, :i], tf.expand_dims(prob, axis=1), tf.zeros([tf.shape(glyph, out_type=tf.int64)[0], max_dup - i - 1], dtype='float32')], axis=1)\n",
    "        return results, probs, dups_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    optimizer_step1 = tf.keras.optimizers.Adam()\n",
    "    optimizer_step2 = tf.keras.optimizers.Adam()\n",
    "    optimizer_length = tf.keras.optimizers.Adam()\n",
    "    optimizer_dups = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    pred_index = tf.math.argmax(pred, axis=-1)\n",
    "    return tf.cast(pred_index == real, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step1(glyph, target, length, total_dups, curr_dups):\n",
    "    # distributed run function kernel\n",
    "    @tf.function\n",
    "    def dist_step(glyph, target, length, total_dups, curr_dups):\n",
    "        sample_loss = tf.zeros([glyph.shape[0]]); sample_accuracy = tf.zeros([glyph.shape[0]])\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = encoder(glyph)\n",
    "            for i in range(1, target.shape[1]):\n",
    "                position = tf.constant(i-1, dtype='int64', shape=[target.shape[0]])\n",
    "                prediction, weight = simple_decoder([features, position])\n",
    "                sample_loss += loss_function(target[:, i], prediction)\n",
    "                sample_accuracy += accuracy_function(target[:, i], prediction)\n",
    "            loss = tf.reduce_sum(sample_loss) / BATCH_SIZE\n",
    "\n",
    "        trainable_variables = simple_decoder.trainable_variables + encoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer_step1.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape_length:\n",
    "            length_pred = length_decoder([features, total_dups, curr_dups])\n",
    "            loss_length = tf.reduce_sum(loss_function(tf.math.argmax(length, axis=-1), length_pred)) / BATCH_SIZE\n",
    "\n",
    "        gradients_length = tape_length.gradient(loss_length, length_decoder.trainable_variables)\n",
    "        optimizer_length.apply_gradients(zip(gradients_length, length_decoder.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape_dups:\n",
    "            dups_pred = dup_decoder([features, None, None])\n",
    "            loss_dups = tf.reduce_sum(loss_function(tf.math.argmax(total_dups, axis=-1), dups_pred)) / BATCH_SIZE\n",
    "\n",
    "        gradients_dups = tape_dups.gradient(loss_dups, dup_decoder.trainable_variables)\n",
    "        optimizer_dups.apply_gradients(zip(gradients_dups, dup_decoder.trainable_variables))\n",
    "\n",
    "        return sample_loss / (target.shape[1] - 1), sample_accuracy / (target.shape[1] - 1)\n",
    "\n",
    "    sample_loss, sample_accuracy = strategy.run(dist_step, args=(glyph, target, length, total_dups, curr_dups))\n",
    "    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_loss, axis=0)\n",
    "    accuracy = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy, axis=0)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step1(glyph, target):\n",
    "    @tf.function\n",
    "    def dist_step(glyph, target):\n",
    "        sample_loss = tf.zeros([glyph.shape[0]]); sample_accuracy = tf.zeros([glyph.shape[0]])\n",
    "        feature = encoder(glyph, training=False)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            position = tf.constant(i-1, dtype='int64', shape=[target.shape[0]])\n",
    "            prediction, weight = simple_decoder([feature, position])\n",
    "            sample_loss += loss_function(target[:, i], prediction)\n",
    "            sample_accuracy += accuracy_function(target[:, i], prediction)\n",
    "        return sample_loss / (target.shape[1] - 1), sample_accuracy / (target.shape[1] - 1)\n",
    "\n",
    "    sample_loss, sample_accuracy = strategy.run(dist_step, args=(glyph, target))\n",
    "    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_loss, axis=0)\n",
    "    accuracy = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy, axis=0)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step1(epoch):\n",
    "    start = time.time()\n",
    "    total_loss = 0; val_loss = 0\n",
    "    total_accuracy = 0; val_accuracy = 0\n",
    "\n",
    "    for (batch, (glyph_tensor, target, length, total_dups, curr_dups)) in enumerate(dataset, start=1):\n",
    "        t_loss, accuracy = train_step1(glyph_tensor, target, length, total_dups, curr_dups)\n",
    "        total_loss += t_loss\n",
    "        total_accuracy += accuracy\n",
    "        nums = min(num_samples, batch * BATCH_SIZE)\n",
    "        print('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%}; progress {:.1%}, taken {:.0f} sec'.format(\n",
    "            epoch + 1, total_loss/nums, total_accuracy / nums, nums / num_samples, time.time() - start), end='\\r')\n",
    "    \n",
    "    for (glyph_tensor, target, _, _, _) in val_dataset:\n",
    "        t_loss, accuracy = validation_step1(glyph_tensor, target)\n",
    "        val_loss += t_loss\n",
    "        val_accuracy += accuracy\n",
    "   \n",
    "    # storing the epoch end loss value to plot later \n",
    "    with strategy.scope():\n",
    "        ckpt_manager_step1.save()\n",
    "\n",
    "    print ('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%} | Validation Loss {:.4f}, Accuracy {:.2%}; taken {:.0f} sec'.format(\n",
    "        epoch+1, total_loss/num_samples, total_accuracy/num_samples, val_loss/num_samples_val, val_accuracy/num_samples_val, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, max_length, length, total_dups, curr_dups):\n",
    "    # start with 0\n",
    "    dec_input = tf.constant([[[1] + [0] * (VOCAB - 1)]] * features.shape[0], dtype='float32')\n",
    "    hidden = decoder.reset_state(batch_size=features.shape[0])\n",
    "    probability = tf.ones([features.shape[0]], dtype='float32')\n",
    "    # iterate predictions, no teacher forcing here\n",
    "    for i in range(max_length):\n",
    "        prediction, hidden, attention_weights = decoder(\n",
    "            [tf.expand_dims(dec_input[:, i, :], 1), length, total_dups, curr_dups, features, hidden], training=False, teacher_forcing=False)\n",
    "        # we need deterministic result\n",
    "        prediction = tf.math.softmax(prediction, axis=-1)\n",
    "        probability *= tf.math.reduce_max(prediction, axis=-1)\n",
    "        dec_input = tf.concat([dec_input, tf.expand_dims(prediction, 1)], axis=1)\n",
    "    return tf.math.argmax(dec_input, axis=-1), probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(features, target, length, total_dups, curr_dups, training=True, teacher_forcing=True):\n",
    "    hidden = decoder.reset_state(batch_size=features.shape[0])\n",
    "    predictions = tf.zeros([features.shape[0], 1, VOCAB], dtype='float32')\n",
    "    if not teacher_forcing:\n",
    "        previous = tf.constant([[[1] + [0] * (VOCAB - 1)]] * features.shape[0], dtype='float32')\n",
    "    for i in range(target.shape[1]-1):\n",
    "        if teacher_forcing:\n",
    "            previous = tf.expand_dims(target[:, i], 1)\n",
    "        prediction, hidden, attention_weights = decoder(\n",
    "            [previous, length, total_dups, curr_dups, features, hidden], training=training, teacher_forcing=teacher_forcing)\n",
    "        predictions = tf.concat([predictions, tf.expand_dims(prediction, 1)], axis=1)\n",
    "        if not teacher_forcing:\n",
    "            previous = tf.expand_dims(tf.math.softmax(prediction, axis=-1), 1)\n",
    "    return predictions[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_step2(real, pred):\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred)\n",
    "    return tf.reduce_sum(loss_, axis=1)\n",
    "    \n",
    "def accuracy_function_step2(real, pred):\n",
    "    accuracy = tf.math.reduce_all(pred == real, axis=1)\n",
    "    return tf.cast(accuracy, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step2(glyph_tensor, target, length, total_dups, curr_dups, teacher_forcing=True):\n",
    "    @tf.function\n",
    "    def dist_step(glyph_tensor, target, length, total_dups, curr_dups, teacher_forcing):\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = encoder(glyph_tensor)\n",
    "\n",
    "            with tf.GradientTape() as tape_dups:\n",
    "                dups_pred = dup_decoder([features, None, None])\n",
    "                loss_dups = tf.reduce_sum(loss_function(tf.math.argmax(total_dups, axis=-1), dups_pred)) / BATCH_SIZE\n",
    "            dups_pred = tf.nn.softmax(dups_pred, axis=-1)\n",
    "\n",
    "            with tf.GradientTape() as tape_length:\n",
    "                if teacher_forcing:\n",
    "                    length_pred = length_decoder([features, total_dups, curr_dups])\n",
    "                else:\n",
    "                    length_pred = length_decoder([features, dups_pred, curr_dups])\n",
    "                loss_length = tf.reduce_sum(loss_function(tf.math.argmax(length, axis=-1), length_pred)) / BATCH_SIZE\n",
    "            length_pred = tf.nn.softmax(length_pred, axis=-1)\n",
    "\n",
    "            if teacher_forcing:\n",
    "                predictions = predict_next(features, target, length, total_dups, curr_dups, teacher_forcing=teacher_forcing)\n",
    "            else:\n",
    "                predictions = predict_next(features, target, length_pred, dups_pred, curr_dups, teacher_forcing=teacher_forcing)\n",
    "            sample_loss = loss_function_step2(target[:, 1:], predictions)\n",
    "            loss = tf.reduce_sum(sample_loss) / BATCH_SIZE\n",
    "\n",
    "        trainable_variables = decoder.trainable_variables + encoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer_step2.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        gradients_length = tape_length.gradient(loss_length, length_decoder.trainable_variables)\n",
    "        optimizer_length.apply_gradients(zip(gradients_length, length_decoder.trainable_variables))\n",
    "\n",
    "        gradients_dups = tape_dups.gradient(loss_dups, dup_decoder.trainable_variables)\n",
    "        optimizer_dups.apply_gradients(zip(gradients_dups, dup_decoder.trainable_variables))\n",
    "\n",
    "        # calculate accuracy based on the code's whole string\n",
    "        predictions_id, _ = predict(features, MAX_LEN, length_pred, dups_pred, curr_dups)\n",
    "        sample_accuracy = accuracy_function_step2(predictions_id, target)\n",
    "        sample_accuracy_length = accuracy_function(tf.math.argmax(length, axis=-1), length_pred)\n",
    "        sample_accuracy_dups = accuracy_function(tf.math.argmax(total_dups, axis=-1), dups_pred)\n",
    "\n",
    "        return sample_loss / (target.shape[1] - 1), sample_accuracy, sample_accuracy_length, sample_accuracy_dups\n",
    "    \n",
    "    sample_loss, sample_accuracy, sample_accuracy_length, sample_accuracy_dups = strategy.run(dist_step, args=(glyph_tensor, target, length, total_dups, curr_dups, teacher_forcing))\n",
    "    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_loss, axis=0)\n",
    "    accuracy = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy, axis=0)\n",
    "    accuracy_length = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy_length, axis=0)\n",
    "    accuracy_dups = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy_dups, axis=0)\n",
    "    return loss, accuracy, accuracy_length, accuracy_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step2(glyph_tensor, target, length, total_dups, curr_dups, cal_percil=False):\n",
    "    @tf.function\n",
    "    def dist_step(glyph_tensor, target, length, total_dups, curr_dups):\n",
    "        features = encoder(glyph_tensor, training=False)\n",
    "        dups_pred = tf.nn.softmax(dup_decoder([features, None, None]), axis=-1)\n",
    "        length_pred = tf.nn.softmax(length_decoder([features, dups_pred, curr_dups]), axis=-1)\n",
    "        predictions = predict_next(features, target, length_pred, dups_pred, curr_dups, training=False, teacher_forcing=False)\n",
    "        sample_loss = loss_function_step2(target[:, 1:], predictions)\n",
    "\n",
    "        # calculate accuracy based on the code's whole string\n",
    "        predictions_id, probability = predict(features, MAX_LEN, length_pred, dups_pred, curr_dups)\n",
    "        sample_accuracy = accuracy_function_step2(predictions_id, target)\n",
    "        sample_accuracy_length = accuracy_function(tf.math.argmax(length, axis=-1), length_pred)\n",
    "        sample_accuracy_dups = accuracy_function(tf.math.argmax(total_dups, axis=-1), dups_pred)\n",
    "\n",
    "        return sample_loss / (target.shape[1] - 1), sample_accuracy, sample_accuracy_length, sample_accuracy_dups, probability\n",
    "    \n",
    "    sample_loss, sample_accuracy, sample_accuracy_length, sample_accuracy_dups, sample_prob = strategy.run(dist_step, args=(glyph_tensor, target, length, total_dups, curr_dups))\n",
    "    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_loss, axis=0)\n",
    "    accuracy = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy, axis=0)\n",
    "    accuracy_length = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy_length, axis=0)\n",
    "    accuracy_dups = strategy.reduce(tf.distribute.ReduceOp.SUM, sample_accuracy_dups, axis=0)\n",
    "    if cal_percil:\n",
    "        all_accuracy = tf.concat(strategy.experimental_local_results(sample_accuracy), axis=0)\n",
    "        all_prob = tf.concat(strategy.experimental_local_results(sample_prob), axis=0)\n",
    "        return loss, accuracy, accuracy_length, accuracy_dups, all_accuracy, all_prob\n",
    "    else:\n",
    "        return loss, accuracy, accuracy_length, accuracy_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step2(epoch, EPOCH):\n",
    "    start = time.time()\n",
    "    total_loss = 0; val_loss = 0; dups_accu = 0; val_dups_accu = 0\n",
    "    total_accuracy = 0; val_accuracy = 0; len_accu = 0; val_len_accu = 0\n",
    "    \n",
    "    num_steps = num_samples // BATCH_SIZE + min(1, num_samples % BATCH_SIZE)\n",
    "    counts = int(num_steps * max(min(2.5 * epoch / EPOCH - 0.5, 1.0), 0.0))\n",
    "    choices = np.random.choice(range(1, num_steps+1), counts, replace=False)\n",
    "    \n",
    "    for (batch, (glyph_tensor, target, length, total_dups, curr_dups)) in enumerate(dataset, start=1):\n",
    "        teacher_forcing = not batch in choices\n",
    "\n",
    "        t_loss, accuracy, accuracy_length, accuracy_dups = train_step2(glyph_tensor, target, length, total_dups, curr_dups, teacher_forcing=teacher_forcing)\n",
    "        total_loss += t_loss; total_accuracy += accuracy\n",
    "        len_accu += accuracy_length; dups_accu += accuracy_dups\n",
    "        \n",
    "        nums = min(num_samples, batch * BATCH_SIZE)\n",
    "        print('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%}; Length Accuracy {:.2%}, Dups Accuracy {:.2%}; progress {:.1%}, taken {:.0f} sec'.format(\n",
    "            epoch + 1, total_loss/nums, total_accuracy/nums, len_accu/nums, dups_accu/nums, nums/num_samples, time.time() - start), end='\\r')\n",
    "    \n",
    "    for (glyph_tensor, target, length, total_dups, curr_dups) in val_dataset:\n",
    "        t_loss, accuracy, accuracy_length, accuracy_dups = validation_step2(glyph_tensor, target, length, total_dups, curr_dups)\n",
    "        val_loss += t_loss; val_accuracy += accuracy\n",
    "        val_len_accu += accuracy_length; val_dups_accu += accuracy_dups\n",
    "   \n",
    "    # storing the epoch end loss value to plot later\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', (total_loss / num_samples), step=epoch)\n",
    "        tf.summary.scalar('accuracy', (total_accuracy / num_samples), step=epoch)\n",
    "        tf.summary.scalar('length_accuracy', (len_accu / num_samples), step=epoch)\n",
    "        tf.summary.scalar('duplication_accuracy', (dups_accu / num_samples), step=epoch)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_loss / num_samples_val, step=epoch)\n",
    "        tf.summary.scalar('accuracy', val_accuracy / num_samples_val, step=epoch)\n",
    "        tf.summary.scalar('length_accuracy', val_len_accu / num_samples_val, step=epoch)\n",
    "        tf.summary.scalar('duplication_accuracy', val_dups_accu / num_samples_val, step=epoch)\n",
    "    \n",
    "    with strategy.scope():\n",
    "        ckpt_manager_step2.save()\n",
    "\n",
    "    print('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%}; Length Accuracy {:.2%}, Dups Accuracy {:.2%} | Validation Loss {:.4f}, Accuracy {:.2%}; Length Accuracy {:.2%}, Dups Accuracy {:.2%}; taken {:.0f} sec'.format(\n",
    "        epoch + 1, total_loss/num_samples, total_accuracy/num_samples, len_accu/num_samples, dups_accu/num_samples, \n",
    "        val_loss/num_samples_val, val_accuracy/num_samples_val, val_len_accu/num_samples_val, val_dups_accu/num_samples_val, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    encoder = CNN_Encoder(embedding_dim = EBD_DIM)\n",
    "    simple_decoder = Simple_Decoder(embedding_dim = EBD_DIM, max_length = MAX_LEN, hidden_size = UNIT_DIM, vocab_size = VOCAB)\n",
    "    length_decoder = Dense_Decoder(max_length = MAX_LEN)\n",
    "    dup_decoder = Dense_Decoder(max_length = MAX_DUP)\n",
    "    decoder = RNN_Decoder(embedding_dim=EBD_DIM, hidden_size=UNIT_DIM, max_length = MAX_LEN, vocab_size=VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a checkpoint to store weights\n",
    "checkpoint_path_step1 = './checkpoints/train_step1'\n",
    "ckpt_step1 = tf.train.Checkpoint(encoder=encoder, decoder=simple_decoder, length_decoder=length_decoder,\n",
    "    dup_decoder=dup_decoder, optimizer=optimizer_step1, optimizer_length=optimizer_length, optimizer_dups=optimizer_dups)\n",
    "ckpt_manager_step1 = tf.train.CheckpointManager(ckpt_step1, checkpoint_path_step1, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a checkpoint to store weights\n",
    "checkpoint_path_step2 = \"./checkpoints/train_step2\"\n",
    "ckpt_step2 = tf.train.Checkpoint(encoder=encoder, decoder=decoder, length_decoder=length_decoder,\n",
    "    dup_decoder=dup_decoder, optimizer=optimizer_step2, optimizer_length=optimizer_length, optimizer_dups=optimizer_dups)\n",
    "ckpt_manager_step2 = tf.train.CheckpointManager(ckpt_step2, checkpoint_path_step2, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "#graph_log_dir = 'logs/gradient_tape/' + current_time + '/func'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "#graph_summary_writer = tf.summary.create_file_writer(graph_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_STEP1 = 20\n",
    "\n",
    "epoch_step1 = 0\n",
    "if ckpt_manager_step1.latest_checkpoint:\n",
    "    epoch_step1 = int(ckpt_manager_step1.latest_checkpoint.split('-')[-1])\n",
    "    ckpt_step1.restore(ckpt_manager_step1.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1, Train Loss 1.8967, Accuracy 45.99% | Validation Loss 1.6580, Accuracy 57.04%; taken 145 sec\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 2, Train Loss 1.0138, Accuracy 69.82% | Validation Loss 1.1340, Accuracy 67.72%; taken 113 sec\n",
      "Epoch 3, Train Loss 0.8177, Accuracy 75.41% | Validation Loss 0.7725, Accuracy 77.65%; taken 113 sec\n",
      "Epoch 4, Train Loss 0.7082, Accuracy 78.47% | Validation Loss 0.8100, Accuracy 76.37%; taken 113 sec\n",
      "Epoch 5, Train Loss 0.6333, Accuracy 80.62% | Validation Loss 0.7292, Accuracy 77.96%; taken 113 sec\n",
      "Epoch 6, Train Loss 0.5806, Accuracy 82.12% | Validation Loss 0.6871, Accuracy 79.00%; taken 113 sec\n",
      "Epoch 7, Train Loss 0.5399, Accuracy 83.29% | Validation Loss 0.6700, Accuracy 79.77%; taken 113 sec\n",
      "Epoch 8, Train Loss 0.5059, Accuracy 84.26% | Validation Loss 0.5501, Accuracy 82.91%; taken 112 sec\n",
      "Epoch 9, Train Loss 0.4789, Accuracy 85.07% | Validation Loss 0.5367, Accuracy 83.66%; taken 113 sec\n",
      "Epoch 10, Train Loss 0.4552, Accuracy 85.74% | Validation Loss 0.4921, Accuracy 85.15%; taken 112 sec\n",
      "Epoch 11, Train Loss 0.4363, Accuracy 86.24% | Validation Loss 0.4979, Accuracy 85.28%; taken 113 sec\n",
      "Epoch 12, Train Loss 0.4189, Accuracy 86.75% | Validation Loss 0.4680, Accuracy 85.41%; taken 112 sec\n",
      "Epoch 13, Train Loss 0.4007, Accuracy 87.29% | Validation Loss 0.4758, Accuracy 86.25%; taken 113 sec\n",
      "Epoch 14, Train Loss 0.3883, Accuracy 87.65% | Validation Loss 0.4411, Accuracy 86.72%; taken 113 sec\n",
      "Epoch 15, Train Loss 0.3762, Accuracy 87.96% | Validation Loss 0.4209, Accuracy 86.96%; taken 112 sec\n",
      "Epoch 16, Train Loss 0.3650, Accuracy 88.31% | Validation Loss 0.4646, Accuracy 86.19%; taken 112 sec\n",
      "Epoch 17, Train Loss 0.3532, Accuracy 88.58% | Validation Loss 0.4878, Accuracy 85.81%; taken 113 sec\n",
      "Epoch 18, Train Loss 0.3441, Accuracy 88.87% | Validation Loss 0.4411, Accuracy 86.70%; taken 112 sec\n",
      "Epoch 19, Train Loss 0.3350, Accuracy 89.14% | Validation Loss 0.4317, Accuracy 86.63%; taken 112 sec\n",
      "Epoch 20, Train Loss 0.3277, Accuracy 89.38% | Validation Loss 0.4425, Accuracy 86.86%; taken 112 sec\n"
     ]
    }
   ],
   "source": [
    "while epoch_step1 < EPOCHS_STEP1:\n",
    "    step1(epoch_step1)\n",
    "    epoch_step1 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_STEP2 = 150\n",
    "\n",
    "epoch_step2 = 0\n",
    "if ckpt_manager_step2.latest_checkpoint:\n",
    "    epoch_step2 = int(ckpt_manager_step2.latest_checkpoint.split('-')[-1])\n",
    "    ckpt_step2.restore(ckpt_manager_step2.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss 0.8488, Accuracy 41.84%; Length Accuracy 95.73%, Dups Accuracy 97.25% | Validation Loss 0.5742, Accuracy 57.45%; Length Accuracy 89.41%, Dups Accuracy 93.59%; taken 181 sec\n",
      "Epoch 2, Train Loss 0.3559, Accuracy 66.66%; Length Accuracy 96.23%, Dups Accuracy 97.71% | Validation Loss 0.4370, Accuracy 67.46%; Length Accuracy 93.21%, Dups Accuracy 94.31%; taken 131 sec\n",
      "Epoch 3, Train Loss 0.2867, Accuracy 72.93%; Length Accuracy 96.49%, Dups Accuracy 97.97% | Validation Loss 0.4897, Accuracy 68.40%; Length Accuracy 91.96%, Dups Accuracy 93.77%; taken 131 sec\n",
      "Epoch 4, Train Loss 0.2483, Accuracy 76.54%; Length Accuracy 96.69%, Dups Accuracy 98.06% | Validation Loss 0.4956, Accuracy 68.10%; Length Accuracy 92.67%, Dups Accuracy 93.52%; taken 131 sec\n",
      "Epoch 5, Train Loss 0.2215, Accuracy 78.91%; Length Accuracy 96.65%, Dups Accuracy 98.04% | Validation Loss 0.4803, Accuracy 72.15%; Length Accuracy 92.31%, Dups Accuracy 93.33%; taken 131 sec\n",
      "Epoch 6, Train Loss 0.2024, Accuracy 80.85%; Length Accuracy 96.88%, Dups Accuracy 98.09% | Validation Loss 0.4399, Accuracy 72.97%; Length Accuracy 92.76%, Dups Accuracy 91.75%; taken 130 sec\n",
      "Epoch 7, Train Loss 0.1862, Accuracy 82.41%; Length Accuracy 96.97%, Dups Accuracy 98.25% | Validation Loss 0.5870, Accuracy 74.30%; Length Accuracy 91.04%, Dups Accuracy 94.56%; taken 131 sec\n",
      "Epoch 8, Train Loss 0.1744, Accuracy 83.39%; Length Accuracy 97.13%, Dups Accuracy 98.40% | Validation Loss 0.5187, Accuracy 75.27%; Length Accuracy 92.53%, Dups Accuracy 95.69%; taken 131 sec\n",
      "Epoch 9, Train Loss 0.1626, Accuracy 84.40%; Length Accuracy 97.20%, Dups Accuracy 98.44% | Validation Loss 0.4984, Accuracy 75.23%; Length Accuracy 93.00%, Dups Accuracy 95.15%; taken 131 sec\n",
      "Epoch 10, Train Loss 0.1526, Accuracy 85.43%; Length Accuracy 97.29%, Dups Accuracy 98.46% | Validation Loss 0.5142, Accuracy 76.24%; Length Accuracy 92.41%, Dups Accuracy 93.77%; taken 131 sec\n",
      "Epoch 11, Train Loss 0.1446, Accuracy 86.19%; Length Accuracy 97.38%, Dups Accuracy 98.52% | Validation Loss 0.4657, Accuracy 76.32%; Length Accuracy 93.09%, Dups Accuracy 93.67%; taken 131 sec\n",
      "Epoch 12, Train Loss 0.1385, Accuracy 86.72%; Length Accuracy 97.36%, Dups Accuracy 98.55% | Validation Loss 0.4525, Accuracy 77.59%; Length Accuracy 93.72%, Dups Accuracy 94.45%; taken 131 sec\n",
      "Epoch 13, Train Loss 0.1299, Accuracy 87.60%; Length Accuracy 97.56%, Dups Accuracy 98.71% | Validation Loss 0.5398, Accuracy 77.29%; Length Accuracy 92.78%, Dups Accuracy 95.25%; taken 131 sec\n",
      "Epoch 14, Train Loss 0.1252, Accuracy 88.04%; Length Accuracy 97.58%, Dups Accuracy 98.63% | Validation Loss 0.4599, Accuracy 78.92%; Length Accuracy 93.64%, Dups Accuracy 94.63%; taken 130 sec\n",
      "Epoch 15, Train Loss 0.1204, Accuracy 88.71%; Length Accuracy 97.76%, Dups Accuracy 98.76% | Validation Loss 0.5026, Accuracy 77.10%; Length Accuracy 93.41%, Dups Accuracy 94.89%; taken 131 sec\n",
      "Epoch 16, Train Loss 0.1145, Accuracy 89.40%; Length Accuracy 97.75%, Dups Accuracy 98.83% | Validation Loss 0.5202, Accuracy 79.88%; Length Accuracy 92.22%, Dups Accuracy 95.44%; taken 131 sec\n",
      "Epoch 17, Train Loss 0.1105, Accuracy 89.75%; Length Accuracy 97.85%, Dups Accuracy 98.82% | Validation Loss 0.5178, Accuracy 78.40%; Length Accuracy 92.74%, Dups Accuracy 95.59%; taken 131 sec\n",
      "Epoch 18, Train Loss 0.1079, Accuracy 90.01%; Length Accuracy 97.85%, Dups Accuracy 98.86% | Validation Loss 0.5039, Accuracy 78.91%; Length Accuracy 93.12%, Dups Accuracy 93.04%; taken 131 sec\n",
      "Epoch 19, Train Loss 0.1020, Accuracy 90.62%; Length Accuracy 97.92%, Dups Accuracy 98.96% | Validation Loss 0.5411, Accuracy 78.96%; Length Accuracy 92.45%, Dups Accuracy 95.10%; taken 131 sec\n",
      "Epoch 20, Train Loss 0.0986, Accuracy 91.00%; Length Accuracy 98.07%, Dups Accuracy 98.91% | Validation Loss 0.5318, Accuracy 81.87%; Length Accuracy 92.96%, Dups Accuracy 95.33%; taken 131 sec\n",
      "Epoch 21, Train Loss 0.0957, Accuracy 91.49%; Length Accuracy 98.13%, Dups Accuracy 98.97% | Validation Loss 0.5673, Accuracy 80.95%; Length Accuracy 91.86%, Dups Accuracy 96.05%; taken 131 sec\n",
      "Epoch 22, Train Loss 0.0933, Accuracy 91.68%; Length Accuracy 98.17%, Dups Accuracy 99.04% | Validation Loss 0.5312, Accuracy 81.72%; Length Accuracy 93.43%, Dups Accuracy 95.15%; taken 130 sec\n",
      "Epoch 23, Train Loss 0.0919, Accuracy 91.77%; Length Accuracy 98.17%, Dups Accuracy 99.04% | Validation Loss 0.6360, Accuracy 79.14%; Length Accuracy 90.94%, Dups Accuracy 95.23%; taken 131 sec\n",
      "Epoch 24, Train Loss 0.0856, Accuracy 92.29%; Length Accuracy 98.20%, Dups Accuracy 99.05% | Validation Loss 0.4601, Accuracy 81.33%; Length Accuracy 94.72%, Dups Accuracy 96.48%; taken 131 sec\n",
      "Epoch 25, Train Loss 0.0858, Accuracy 92.43%; Length Accuracy 98.26%, Dups Accuracy 99.03% | Validation Loss 0.6237, Accuracy 80.35%; Length Accuracy 92.87%, Dups Accuracy 96.76%; taken 131 sec\n",
      "Epoch 26, Train Loss 0.0833, Accuracy 92.75%; Length Accuracy 98.35%, Dups Accuracy 99.15% | Validation Loss 0.5465, Accuracy 81.60%; Length Accuracy 93.31%, Dups Accuracy 94.70%; taken 131 sec\n",
      "Epoch 27, Train Loss 0.0799, Accuracy 93.13%; Length Accuracy 98.37%, Dups Accuracy 99.21% | Validation Loss 0.4395, Accuracy 82.88%; Length Accuracy 94.66%, Dups Accuracy 96.01%; taken 131 sec\n",
      "Epoch 28, Train Loss 0.0791, Accuracy 93.20%; Length Accuracy 98.34%, Dups Accuracy 99.15% | Validation Loss 0.6729, Accuracy 81.61%; Length Accuracy 92.59%, Dups Accuracy 96.54%; taken 131 sec\n",
      "Epoch 29, Train Loss 0.0771, Accuracy 93.37%; Length Accuracy 98.45%, Dups Accuracy 99.24% | Validation Loss 0.7824, Accuracy 79.69%; Length Accuracy 90.10%, Dups Accuracy 96.50%; taken 131 sec\n",
      "Epoch 30, Train Loss 0.0770, Accuracy 93.53%; Length Accuracy 98.46%, Dups Accuracy 99.20% | Validation Loss 0.7444, Accuracy 79.71%; Length Accuracy 91.48%, Dups Accuracy 95.24%; taken 130 sec\n",
      "Epoch 31, Train Loss 0.0745, Accuracy 93.62%; Length Accuracy 98.46%, Dups Accuracy 99.23% | Validation Loss 0.4542, Accuracy 81.74%; Length Accuracy 94.60%, Dups Accuracy 95.58%; taken 130 sec\n",
      "Epoch 32, Train Loss 0.0963, Accuracy 90.96%; Length Accuracy 97.27%, Dups Accuracy 98.60% | Validation Loss 0.6566, Accuracy 80.85%; Length Accuracy 90.39%, Dups Accuracy 96.62%; taken 146 sec\n",
      "Epoch 33, Train Loss 0.0778, Accuracy 93.39%; Length Accuracy 98.15%, Dups Accuracy 99.22% | Validation Loss 0.4977, Accuracy 80.31%; Length Accuracy 92.07%, Dups Accuracy 95.64%; taken 131 sec\n",
      "Epoch 34, Train Loss 0.0800, Accuracy 93.25%; Length Accuracy 98.15%, Dups Accuracy 99.09% | Validation Loss 0.4781, Accuracy 82.24%; Length Accuracy 91.26%, Dups Accuracy 96.40%; taken 131 sec\n",
      "Epoch 35, Train Loss 0.0855, Accuracy 92.75%; Length Accuracy 97.97%, Dups Accuracy 99.05% | Validation Loss 0.3944, Accuracy 81.33%; Length Accuracy 92.52%, Dups Accuracy 96.21%; taken 131 sec\n",
      "Epoch 36, Train Loss 0.0848, Accuracy 93.00%; Length Accuracy 97.94%, Dups Accuracy 99.09% | Validation Loss 0.4637, Accuracy 79.77%; Length Accuracy 90.28%, Dups Accuracy 97.05%; taken 131 sec\n",
      "Epoch 37, Train Loss 0.0799, Accuracy 93.49%; Length Accuracy 98.19%, Dups Accuracy 99.16% | Validation Loss 0.3178, Accuracy 84.32%; Length Accuracy 93.18%, Dups Accuracy 97.45%; taken 131 sec\n",
      "Epoch 38, Train Loss 0.0781, Accuracy 93.86%; Length Accuracy 98.33%, Dups Accuracy 99.21% | Validation Loss 0.3053, Accuracy 83.08%; Length Accuracy 94.34%, Dups Accuracy 96.16%; taken 131 sec\n",
      "Epoch 39, Train Loss 0.0825, Accuracy 93.65%; Length Accuracy 98.23%, Dups Accuracy 99.21% | Validation Loss 0.3777, Accuracy 79.91%; Length Accuracy 91.70%, Dups Accuracy 96.62%; taken 131 sec\n",
      "Epoch 40, Train Loss 0.0852, Accuracy 93.39%; Length Accuracy 98.19%, Dups Accuracy 99.09% | Validation Loss 0.4480, Accuracy 74.89%; Length Accuracy 92.59%, Dups Accuracy 95.90%; taken 131 sec\n",
      "Epoch 41, Train Loss 0.0829, Accuracy 93.65%; Length Accuracy 98.29%, Dups Accuracy 99.26% | Validation Loss 0.6575, Accuracy 68.67%; Length Accuracy 81.47%, Dups Accuracy 96.48%; taken 142 sec\n",
      "Epoch 42, Train Loss 0.0890, Accuracy 93.32%; Length Accuracy 98.14%, Dups Accuracy 99.14% | Validation Loss 0.4267, Accuracy 79.72%; Length Accuracy 89.19%, Dups Accuracy 97.15%; taken 132 sec\n",
      "Epoch 43, Train Loss 0.0762, Accuracy 94.52%; Length Accuracy 98.57%, Dups Accuracy 99.40% | Validation Loss 0.3261, Accuracy 82.45%; Length Accuracy 93.14%, Dups Accuracy 96.39%; taken 131 sec\n",
      "Epoch 44, Train Loss 0.0794, Accuracy 94.21%; Length Accuracy 98.45%, Dups Accuracy 99.33% | Validation Loss 0.2629, Accuracy 84.25%; Length Accuracy 94.67%, Dups Accuracy 96.61%; taken 131 sec\n",
      "Epoch 45, Train Loss 0.0777, Accuracy 94.41%; Length Accuracy 98.56%, Dups Accuracy 99.35% | Validation Loss 0.3254, Accuracy 80.61%; Length Accuracy 93.50%, Dups Accuracy 96.58%; taken 132 sec\n",
      "Epoch 46, Train Loss 0.0778, Accuracy 94.50%; Length Accuracy 98.62%, Dups Accuracy 99.34% | Validation Loss 0.2909, Accuracy 83.49%; Length Accuracy 94.53%, Dups Accuracy 97.36%; taken 132 sec\n",
      "Epoch 47, Train Loss 0.0843, Accuracy 94.09%; Length Accuracy 98.48%, Dups Accuracy 99.26% | Validation Loss 0.2861, Accuracy 83.36%; Length Accuracy 94.15%, Dups Accuracy 96.92%; taken 132 sec\n",
      "Epoch 48, Train Loss 0.0787, Accuracy 94.56%; Length Accuracy 98.60%, Dups Accuracy 99.36% | Validation Loss 0.2911, Accuracy 81.65%; Length Accuracy 94.07%, Dups Accuracy 97.33%; taken 132 sec\n",
      "Epoch 49, Train Loss 0.0804, Accuracy 94.58%; Length Accuracy 98.63%, Dups Accuracy 99.37% | Validation Loss 0.3013, Accuracy 82.66%; Length Accuracy 94.32%, Dups Accuracy 97.50%; taken 132 sec\n",
      "Epoch 50, Train Loss 0.0757, Accuracy 94.89%; Length Accuracy 98.71%, Dups Accuracy 99.45% | Validation Loss 0.2550, Accuracy 84.88%; Length Accuracy 93.83%, Dups Accuracy 97.39%; taken 132 sec\n",
      "Epoch 51, Train Loss 0.0798, Accuracy 94.78%; Length Accuracy 98.76%, Dups Accuracy 99.42% | Validation Loss 0.2883, Accuracy 83.38%; Length Accuracy 94.42%, Dups Accuracy 96.68%; taken 132 sec\n",
      "Epoch 52, Train Loss 0.0780, Accuracy 94.83%; Length Accuracy 98.73%, Dups Accuracy 99.42% | Validation Loss 0.2927, Accuracy 81.96%; Length Accuracy 93.95%, Dups Accuracy 97.59%; taken 131 sec\n",
      "Epoch 53, Train Loss 0.0786, Accuracy 94.88%; Length Accuracy 98.69%, Dups Accuracy 99.42% | Validation Loss 0.2657, Accuracy 83.73%; Length Accuracy 92.86%, Dups Accuracy 97.61%; taken 132 sec\n",
      "Epoch 54, Train Loss 0.0742, Accuracy 95.16%; Length Accuracy 98.91%, Dups Accuracy 99.46% | Validation Loss 0.2702, Accuracy 83.94%; Length Accuracy 93.93%, Dups Accuracy 97.77%; taken 131 sec\n",
      "Epoch 55, Train Loss 0.0876, Accuracy 94.38%; Length Accuracy 98.64%, Dups Accuracy 99.29% | Validation Loss 0.4307, Accuracy 72.18%; Length Accuracy 90.37%, Dups Accuracy 96.94%; taken 132 sec\n",
      "Epoch 56, Train Loss 0.0729, Accuracy 95.42%; Length Accuracy 98.96%, Dups Accuracy 99.53% | Validation Loss 0.2550, Accuracy 85.04%; Length Accuracy 94.31%, Dups Accuracy 97.84%; taken 132 sec\n",
      "Epoch 57, Train Loss 0.0732, Accuracy 95.50%; Length Accuracy 98.97%, Dups Accuracy 99.49% | Validation Loss 0.2426, Accuracy 85.62%; Length Accuracy 95.22%, Dups Accuracy 97.40%; taken 132 sec\n",
      "Epoch 58, Train Loss 0.0799, Accuracy 94.97%; Length Accuracy 98.75%, Dups Accuracy 99.35% | Validation Loss 0.2459, Accuracy 84.55%; Length Accuracy 94.75%, Dups Accuracy 97.46%; taken 132 sec\n",
      "Epoch 59, Train Loss 0.0741, Accuracy 95.40%; Length Accuracy 98.97%, Dups Accuracy 99.53% | Validation Loss 0.2906, Accuracy 83.46%; Length Accuracy 93.53%, Dups Accuracy 96.96%; taken 132 sec\n",
      "Epoch 60, Train Loss 0.0820, Accuracy 94.94%; Length Accuracy 98.87%, Dups Accuracy 99.46% | Validation Loss 0.2871, Accuracy 82.68%; Length Accuracy 92.59%, Dups Accuracy 97.38%; taken 132 sec\n",
      "Epoch 61, Train Loss 0.0725, Accuracy 95.44%; Length Accuracy 98.93%, Dups Accuracy 99.47% | Validation Loss 0.2669, Accuracy 84.61%; Length Accuracy 93.78%, Dups Accuracy 97.54%; taken 132 sec\n",
      "Epoch 62, Train Loss 0.0720, Accuracy 95.78%; Length Accuracy 99.02%, Dups Accuracy 99.55% | Validation Loss 0.3006, Accuracy 80.35%; Length Accuracy 94.07%, Dups Accuracy 97.38%; taken 132 sec\n",
      "Epoch 63, Train Loss 0.0705, Accuracy 95.79%; Length Accuracy 99.08%, Dups Accuracy 99.57% | Validation Loss 0.2673, Accuracy 84.30%; Length Accuracy 93.30%, Dups Accuracy 97.65%; taken 132 sec\n",
      "Epoch 64, Train Loss 0.0731, Accuracy 95.66%; Length Accuracy 99.05%, Dups Accuracy 99.54% | Validation Loss 0.2945, Accuracy 82.74%; Length Accuracy 92.72%, Dups Accuracy 97.48%; taken 132 sec\n",
      "Epoch 65, Train Loss 0.0729, Accuracy 95.65%; Length Accuracy 98.99%, Dups Accuracy 99.60% | Validation Loss 0.2473, Accuracy 85.39%; Length Accuracy 94.29%, Dups Accuracy 97.45%; taken 133 sec\n",
      "Epoch 66, Train Loss 0.0738, Accuracy 95.75%; Length Accuracy 99.06%, Dups Accuracy 99.46% | Validation Loss 0.2523, Accuracy 83.73%; Length Accuracy 94.47%, Dups Accuracy 97.51%; taken 133 sec\n",
      "Epoch 67, Train Loss 0.0728, Accuracy 95.84%; Length Accuracy 99.07%, Dups Accuracy 99.56% | Validation Loss 0.2693, Accuracy 83.74%; Length Accuracy 94.15%, Dups Accuracy 97.48%; taken 133 sec\n",
      "Epoch 68, Train Loss 0.0734, Accuracy 95.71%; Length Accuracy 99.06%, Dups Accuracy 99.56% | Validation Loss 0.3011, Accuracy 82.59%; Length Accuracy 92.99%, Dups Accuracy 97.44%; taken 132 sec\n",
      "Epoch 69, Train Loss 0.0739, Accuracy 95.75%; Length Accuracy 99.08%, Dups Accuracy 99.56% | Validation Loss 0.2455, Accuracy 84.83%; Length Accuracy 94.66%, Dups Accuracy 97.49%; taken 133 sec\n",
      "Epoch 70, Train Loss 0.0659, Accuracy 96.19%; Length Accuracy 99.20%, Dups Accuracy 99.62% | Validation Loss 0.2452, Accuracy 85.40%; Length Accuracy 94.61%, Dups Accuracy 97.84%; taken 132 sec\n",
      "Epoch 71, Train Loss 0.0702, Accuracy 96.08%; Length Accuracy 99.16%, Dups Accuracy 99.58% | Validation Loss 0.3305, Accuracy 77.04%; Length Accuracy 93.32%, Dups Accuracy 97.12%; taken 133 sec\n",
      "Epoch 72, Train Loss 0.0703, Accuracy 96.00%; Length Accuracy 99.12%, Dups Accuracy 99.59% | Validation Loss 0.2817, Accuracy 81.50%; Length Accuracy 91.75%, Dups Accuracy 97.90%; taken 133 sec\n",
      "Epoch 73, Train Loss 0.0718, Accuracy 95.95%; Length Accuracy 99.11%, Dups Accuracy 99.56% | Validation Loss 0.2472, Accuracy 84.42%; Length Accuracy 94.47%, Dups Accuracy 97.54%; taken 133 sec\n",
      "Epoch 74, Train Loss 0.0689, Accuracy 96.18%; Length Accuracy 99.20%, Dups Accuracy 99.63% | Validation Loss 0.2745, Accuracy 84.42%; Length Accuracy 92.95%, Dups Accuracy 97.72%; taken 133 sec\n",
      "Epoch 75, Train Loss 0.0678, Accuracy 96.30%; Length Accuracy 99.19%, Dups Accuracy 99.65% | Validation Loss 0.2449, Accuracy 84.88%; Length Accuracy 94.31%, Dups Accuracy 97.38%; taken 133 sec\n",
      "Epoch 76, Train Loss 0.0668, Accuracy 96.29%; Length Accuracy 99.23%, Dups Accuracy 99.69% | Validation Loss 0.2825, Accuracy 80.24%; Length Accuracy 94.37%, Dups Accuracy 97.74%; taken 133 sec\n",
      "Epoch 77, Train Loss 0.0709, Accuracy 96.12%; Length Accuracy 99.13%, Dups Accuracy 99.55% | Validation Loss 0.2694, Accuracy 84.03%; Length Accuracy 92.01%, Dups Accuracy 97.61%; taken 133 sec\n",
      "Epoch 78, Train Loss 0.0664, Accuracy 96.41%; Length Accuracy 99.33%, Dups Accuracy 99.64% | Validation Loss 0.2432, Accuracy 84.93%; Length Accuracy 94.63%, Dups Accuracy 97.39%; taken 133 sec\n",
      "Epoch 79, Train Loss 0.0683, Accuracy 96.27%; Length Accuracy 99.22%, Dups Accuracy 99.68% | Validation Loss 0.2634, Accuracy 84.17%; Length Accuracy 94.18%, Dups Accuracy 97.69%; taken 134 sec\n",
      "Epoch 80, Train Loss 0.0727, Accuracy 96.13%; Length Accuracy 99.19%, Dups Accuracy 99.56% | Validation Loss 0.2308, Accuracy 84.88%; Length Accuracy 95.22%, Dups Accuracy 97.61%; taken 133 sec\n",
      "Epoch 81, Train Loss 0.0662, Accuracy 96.43%; Length Accuracy 99.32%, Dups Accuracy 99.64% | Validation Loss 0.2499, Accuracy 84.84%; Length Accuracy 94.98%, Dups Accuracy 97.79%; taken 133 sec\n",
      "Epoch 82, Train Loss 0.0625, Accuracy 96.71%; Length Accuracy 99.39%, Dups Accuracy 99.70% | Validation Loss 0.3417, Accuracy 82.03%; Length Accuracy 90.77%, Dups Accuracy 97.70%; taken 133 sec\n",
      "Epoch 83, Train Loss 0.0669, Accuracy 96.46%; Length Accuracy 99.28%, Dups Accuracy 99.64% | Validation Loss 0.2580, Accuracy 82.97%; Length Accuracy 94.70%, Dups Accuracy 97.28%; taken 134 sec\n",
      "Epoch 84, Train Loss 0.0686, Accuracy 96.36%; Length Accuracy 99.22%, Dups Accuracy 99.64% | Validation Loss 0.2439, Accuracy 85.44%; Length Accuracy 95.19%, Dups Accuracy 97.91%; taken 133 sec\n",
      "Epoch 85, Train Loss 0.0640, Accuracy 96.73%; Length Accuracy 99.34%, Dups Accuracy 99.62% | Validation Loss 0.2473, Accuracy 85.09%; Length Accuracy 93.41%, Dups Accuracy 97.70%; taken 134 sec\n",
      "Epoch 86, Train Loss 0.0623, Accuracy 96.78%; Length Accuracy 99.32%, Dups Accuracy 99.67% | Validation Loss 0.2379, Accuracy 84.02%; Length Accuracy 94.84%, Dups Accuracy 98.05%; taken 133 sec\n",
      "Epoch 87, Train Loss 0.0664, Accuracy 96.55%; Length Accuracy 99.30%, Dups Accuracy 99.66% | Validation Loss 0.2474, Accuracy 85.67%; Length Accuracy 93.87%, Dups Accuracy 98.16%; taken 134 sec\n",
      "Epoch 88, Train Loss 0.0658, Accuracy 96.57%; Length Accuracy 99.33%, Dups Accuracy 99.63% | Validation Loss 0.2626, Accuracy 84.24%; Length Accuracy 92.89%, Dups Accuracy 97.70%; taken 134 sec\n",
      "Epoch 89, Train Loss 0.0642, Accuracy 96.79%; Length Accuracy 99.35%, Dups Accuracy 99.70% | Validation Loss 0.2535, Accuracy 83.73%; Length Accuracy 94.76%, Dups Accuracy 97.93%; taken 134 sec\n",
      "Epoch 90, Train Loss 0.0704, Accuracy 96.42%; Length Accuracy 99.25%, Dups Accuracy 99.65% | Validation Loss 0.2654, Accuracy 81.35%; Length Accuracy 94.77%, Dups Accuracy 97.53%; taken 133 sec\n",
      "Epoch 91, Train Loss 0.0671, Accuracy 96.63%; Length Accuracy 99.26%, Dups Accuracy 99.66% | Validation Loss 0.2238, Accuracy 86.17%; Length Accuracy 95.43%, Dups Accuracy 98.02%; taken 134 sec\n",
      "Epoch 92, Train Loss 0.0629, Accuracy 96.83%; Length Accuracy 99.35%, Dups Accuracy 99.64% | Validation Loss 0.2633, Accuracy 84.84%; Length Accuracy 93.83%, Dups Accuracy 98.11%; taken 133 sec\n",
      "Epoch 93, Train Loss 0.0630, Accuracy 96.88%; Length Accuracy 99.40%, Dups Accuracy 99.70% | Validation Loss 0.3693, Accuracy 76.61%; Length Accuracy 91.36%, Dups Accuracy 97.63%; taken 134 sec\n",
      "Epoch 94, Train Loss 0.0601, Accuracy 97.02%; Length Accuracy 99.42%, Dups Accuracy 99.74% | Validation Loss 0.2418, Accuracy 86.19%; Length Accuracy 94.68%, Dups Accuracy 97.99%; taken 133 sec\n",
      "Epoch 95, Train Loss 0.0617, Accuracy 96.92%; Length Accuracy 99.36%, Dups Accuracy 99.74% | Validation Loss 0.2383, Accuracy 86.05%; Length Accuracy 94.83%, Dups Accuracy 97.95%; taken 134 sec\n",
      "Epoch 96, Train Loss 0.0617, Accuracy 96.92%; Length Accuracy 99.39%, Dups Accuracy 99.64% | Validation Loss 0.2341, Accuracy 85.17%; Length Accuracy 94.89%, Dups Accuracy 97.83%; taken 134 sec\n",
      "Epoch 97, Train Loss 0.0589, Accuracy 97.05%; Length Accuracy 99.41%, Dups Accuracy 99.72% | Validation Loss 0.2405, Accuracy 85.31%; Length Accuracy 95.47%, Dups Accuracy 98.06%; taken 134 sec\n",
      "Epoch 98, Train Loss 0.0589, Accuracy 97.13%; Length Accuracy 99.41%, Dups Accuracy 99.72% | Validation Loss 0.2812, Accuracy 83.64%; Length Accuracy 93.36%, Dups Accuracy 97.78%; taken 133 sec\n",
      "Epoch 99, Train Loss 0.0636, Accuracy 96.83%; Length Accuracy 99.35%, Dups Accuracy 99.66% | Validation Loss 0.2287, Accuracy 85.94%; Length Accuracy 95.39%, Dups Accuracy 97.85%; taken 134 sec\n",
      "Epoch 100, Train Loss 0.0617, Accuracy 96.93%; Length Accuracy 99.37%, Dups Accuracy 99.67% | Validation Loss 0.2405, Accuracy 85.65%; Length Accuracy 94.84%, Dups Accuracy 98.12%; taken 133 sec\n",
      "Epoch 101, Train Loss 0.0549, Accuracy 97.33%; Length Accuracy 99.53%, Dups Accuracy 99.78% | Validation Loss 0.2404, Accuracy 86.43%; Length Accuracy 94.68%, Dups Accuracy 98.06%; taken 134 sec\n",
      "Epoch 102, Train Loss 0.0556, Accuracy 97.25%; Length Accuracy 99.50%, Dups Accuracy 99.72% | Validation Loss 0.2999, Accuracy 84.30%; Length Accuracy 93.13%, Dups Accuracy 98.04%; taken 133 sec\n",
      "Epoch 103, Train Loss 0.0647, Accuracy 96.89%; Length Accuracy 99.34%, Dups Accuracy 99.61% | Validation Loss 0.2702, Accuracy 83.58%; Length Accuracy 94.79%, Dups Accuracy 98.06%; taken 134 sec\n",
      "Epoch 104, Train Loss 0.0592, Accuracy 97.16%; Length Accuracy 99.42%, Dups Accuracy 99.73% | Validation Loss 0.2737, Accuracy 84.00%; Length Accuracy 92.92%, Dups Accuracy 97.92%; taken 134 sec\n",
      "Epoch 105, Train Loss 0.0577, Accuracy 97.09%; Length Accuracy 99.43%, Dups Accuracy 99.74% | Validation Loss 0.2542, Accuracy 85.56%; Length Accuracy 94.59%, Dups Accuracy 98.17%; taken 134 sec\n",
      "Epoch 106, Train Loss 0.0564, Accuracy 97.26%; Length Accuracy 99.45%, Dups Accuracy 99.76% | Validation Loss 0.2333, Accuracy 86.62%; Length Accuracy 95.42%, Dups Accuracy 97.78%; taken 133 sec\n",
      "Epoch 107, Train Loss 0.0554, Accuracy 97.37%; Length Accuracy 99.47%, Dups Accuracy 99.74% | Validation Loss 0.2560, Accuracy 85.12%; Length Accuracy 94.29%, Dups Accuracy 97.55%; taken 134 sec\n",
      "Epoch 108, Train Loss 0.0545, Accuracy 97.34%; Length Accuracy 99.43%, Dups Accuracy 99.70% | Validation Loss 0.2311, Accuracy 86.90%; Length Accuracy 94.99%, Dups Accuracy 97.99%; taken 133 sec\n",
      "Epoch 109, Train Loss 0.0587, Accuracy 97.12%; Length Accuracy 99.48%, Dups Accuracy 99.72% | Validation Loss 0.2399, Accuracy 86.02%; Length Accuracy 94.87%, Dups Accuracy 97.87%; taken 134 sec\n",
      "Epoch 110, Train Loss 0.0559, Accuracy 97.24%; Length Accuracy 99.41%, Dups Accuracy 99.74% | Validation Loss 0.2269, Accuracy 86.54%; Length Accuracy 95.52%, Dups Accuracy 98.05%; taken 133 sec\n",
      "Epoch 111, Train Loss 0.0539, Accuracy 97.40%; Length Accuracy 99.50%, Dups Accuracy 99.74% | Validation Loss 0.2792, Accuracy 84.89%; Length Accuracy 92.83%, Dups Accuracy 97.91%; taken 134 sec\n",
      "Epoch 112, Train Loss 0.0505, Accuracy 97.60%; Length Accuracy 99.61%, Dups Accuracy 99.71% | Validation Loss 0.3111, Accuracy 81.95%; Length Accuracy 94.50%, Dups Accuracy 97.80%; taken 134 sec\n",
      "Epoch 113, Train Loss 0.0575, Accuracy 97.24%; Length Accuracy 99.42%, Dups Accuracy 99.71% | Validation Loss 0.2464, Accuracy 85.97%; Length Accuracy 95.10%, Dups Accuracy 98.03%; taken 134 sec\n",
      "Epoch 114, Train Loss 0.0636, Accuracy 96.88%; Length Accuracy 99.35%, Dups Accuracy 99.66% | Validation Loss 0.2378, Accuracy 86.34%; Length Accuracy 94.55%, Dups Accuracy 98.31%; taken 133 sec\n",
      "Epoch 115, Train Loss 0.0548, Accuracy 97.33%; Length Accuracy 99.52%, Dups Accuracy 99.77% | Validation Loss 0.2666, Accuracy 85.68%; Length Accuracy 93.36%, Dups Accuracy 98.00%; taken 134 sec\n",
      "Epoch 116, Train Loss 0.0521, Accuracy 97.54%; Length Accuracy 99.52%, Dups Accuracy 99.76% | Validation Loss 0.3016, Accuracy 83.55%; Length Accuracy 91.48%, Dups Accuracy 98.15%; taken 133 sec\n",
      "Epoch 117, Train Loss 0.0541, Accuracy 97.37%; Length Accuracy 99.44%, Dups Accuracy 99.77% | Validation Loss 0.2565, Accuracy 85.09%; Length Accuracy 94.38%, Dups Accuracy 98.00%; taken 134 sec\n",
      "Epoch 118, Train Loss 0.0585, Accuracy 97.22%; Length Accuracy 99.43%, Dups Accuracy 99.73% | Validation Loss 0.2474, Accuracy 86.07%; Length Accuracy 94.53%, Dups Accuracy 98.18%; taken 133 sec\n",
      "Epoch 119, Train Loss 0.0510, Accuracy 97.54%; Length Accuracy 99.55%, Dups Accuracy 99.80% | Validation Loss 0.2508, Accuracy 85.75%; Length Accuracy 94.91%, Dups Accuracy 97.94%; taken 134 sec\n",
      "Epoch 120, Train Loss 0.0553, Accuracy 97.42%; Length Accuracy 99.49%, Dups Accuracy 99.73% | Validation Loss 0.2564, Accuracy 85.16%; Length Accuracy 94.62%, Dups Accuracy 98.09%; taken 134 sec\n",
      "Epoch 121, Train Loss 0.0538, Accuracy 97.41%; Length Accuracy 99.48%, Dups Accuracy 99.80% | Validation Loss 0.2645, Accuracy 83.99%; Length Accuracy 94.61%, Dups Accuracy 97.44%; taken 134 sec\n",
      "Epoch 122, Train Loss 0.0543, Accuracy 97.40%; Length Accuracy 99.48%, Dups Accuracy 99.75% | Validation Loss 0.2921, Accuracy 84.77%; Length Accuracy 94.23%, Dups Accuracy 97.66%; taken 134 sec\n",
      "Epoch 123, Train Loss 0.0523, Accuracy 97.49%; Length Accuracy 99.51%, Dups Accuracy 99.75% | Validation Loss 0.2909, Accuracy 84.84%; Length Accuracy 93.29%, Dups Accuracy 98.03%; taken 134 sec\n",
      "Epoch 124, Train Loss 0.0543, Accuracy 97.45%; Length Accuracy 99.49%, Dups Accuracy 99.73% | Validation Loss 0.2461, Accuracy 86.13%; Length Accuracy 95.24%, Dups Accuracy 98.05%; taken 133 sec\n",
      "Epoch 125, Train Loss 0.0527, Accuracy 97.61%; Length Accuracy 99.55%, Dups Accuracy 99.81% | Validation Loss 0.2769, Accuracy 85.12%; Length Accuracy 93.79%, Dups Accuracy 97.86%; taken 134 sec\n",
      "Epoch 126, Train Loss 0.0500, Accuracy 97.66%; Length Accuracy 99.52%, Dups Accuracy 99.78% | Validation Loss 0.2568, Accuracy 85.65%; Length Accuracy 93.97%, Dups Accuracy 98.04%; taken 134 sec\n",
      "Epoch 127, Train Loss 0.0506, Accuracy 97.61%; Length Accuracy 99.53%, Dups Accuracy 99.77% | Validation Loss 0.2396, Accuracy 85.76%; Length Accuracy 95.09%, Dups Accuracy 97.91%; taken 134 sec\n",
      "Epoch 128, Train Loss 0.0552, Accuracy 97.39%; Length Accuracy 99.49%, Dups Accuracy 99.69% | Validation Loss 0.2572, Accuracy 83.27%; Length Accuracy 94.65%, Dups Accuracy 97.77%; taken 134 sec\n",
      "Epoch 129, Train Loss 0.0531, Accuracy 97.51%; Length Accuracy 99.53%, Dups Accuracy 99.79% | Validation Loss 0.2619, Accuracy 85.20%; Length Accuracy 94.75%, Dups Accuracy 97.79%; taken 134 sec\n",
      "Epoch 130, Train Loss 0.0513, Accuracy 97.55%; Length Accuracy 99.52%, Dups Accuracy 99.79% | Validation Loss 0.2452, Accuracy 86.71%; Length Accuracy 94.72%, Dups Accuracy 98.00%; taken 134 sec\n",
      "Epoch 131, Train Loss 0.0512, Accuracy 97.55%; Length Accuracy 99.55%, Dups Accuracy 99.77% | Validation Loss 0.2408, Accuracy 86.43%; Length Accuracy 94.96%, Dups Accuracy 97.97%; taken 134 sec\n",
      "Epoch 132, Train Loss 0.0558, Accuracy 97.41%; Length Accuracy 99.44%, Dups Accuracy 99.74% | Validation Loss 0.2583, Accuracy 85.94%; Length Accuracy 94.63%, Dups Accuracy 97.97%; taken 133 sec\n",
      "Epoch 133, Train Loss 0.0480, Accuracy 97.73%; Length Accuracy 99.60%, Dups Accuracy 99.78% | Validation Loss 0.2654, Accuracy 85.81%; Length Accuracy 95.07%, Dups Accuracy 97.92%; taken 134 sec\n",
      "Epoch 134, Train Loss 0.0535, Accuracy 97.56%; Length Accuracy 99.49%, Dups Accuracy 99.74% | Validation Loss 0.2414, Accuracy 85.99%; Length Accuracy 95.10%, Dups Accuracy 97.67%; taken 133 sec\n",
      "Epoch 135, Train Loss 0.0469, Accuracy 97.80%; Length Accuracy 99.60%, Dups Accuracy 99.85% | Validation Loss 0.2777, Accuracy 85.40%; Length Accuracy 93.43%, Dups Accuracy 98.32%; taken 133 sec\n",
      "Epoch 136, Train Loss 0.0527, Accuracy 97.53%; Length Accuracy 99.47%, Dups Accuracy 99.76% | Validation Loss 0.2520, Accuracy 85.19%; Length Accuracy 95.12%, Dups Accuracy 98.08%; taken 134 sec\n",
      "Epoch 137, Train Loss 0.0494, Accuracy 97.72%; Length Accuracy 99.63%, Dups Accuracy 99.79% | Validation Loss 0.2462, Accuracy 86.75%; Length Accuracy 95.32%, Dups Accuracy 98.26%; taken 134 sec\n",
      "Epoch 138, Train Loss 0.0472, Accuracy 97.88%; Length Accuracy 99.62%, Dups Accuracy 99.79% | Validation Loss 0.2824, Accuracy 82.84%; Length Accuracy 94.76%, Dups Accuracy 98.00%; taken 134 sec\n",
      "Epoch 139, Train Loss 0.0537, Accuracy 97.48%; Length Accuracy 99.51%, Dups Accuracy 99.72% | Validation Loss 0.2461, Accuracy 86.33%; Length Accuracy 95.27%, Dups Accuracy 98.07%; taken 134 sec\n",
      "Epoch 140, Train Loss 0.0452, Accuracy 97.87%; Length Accuracy 99.60%, Dups Accuracy 99.81% | Validation Loss 0.2766, Accuracy 84.82%; Length Accuracy 94.66%, Dups Accuracy 97.85%; taken 133 sec\n",
      "Epoch 141, Train Loss 0.0502, Accuracy 97.71%; Length Accuracy 99.55%, Dups Accuracy 99.78% | Validation Loss 0.2546, Accuracy 86.50%; Length Accuracy 95.43%, Dups Accuracy 98.11%; taken 134 sec\n",
      "Epoch 142, Train Loss 0.0512, Accuracy 97.58%; Length Accuracy 99.52%, Dups Accuracy 99.75% | Validation Loss 0.2758, Accuracy 85.46%; Length Accuracy 94.53%, Dups Accuracy 97.73%; taken 134 sec\n",
      "Epoch 143, Train Loss 0.0481, Accuracy 97.82%; Length Accuracy 99.58%, Dups Accuracy 99.78% | Validation Loss 0.2539, Accuracy 86.26%; Length Accuracy 94.63%, Dups Accuracy 98.37%; taken 133 sec\n",
      "Epoch 144, Train Loss 0.0517, Accuracy 97.61%; Length Accuracy 99.50%, Dups Accuracy 99.77% | Validation Loss 0.2517, Accuracy 85.95%; Length Accuracy 94.99%, Dups Accuracy 98.34%; taken 134 sec\n",
      "Epoch 145, Train Loss 0.0464, Accuracy 97.81%; Length Accuracy 99.55%, Dups Accuracy 99.80% | Validation Loss 0.3267, Accuracy 82.97%; Length Accuracy 94.37%, Dups Accuracy 98.01%; taken 134 sec\n",
      "Epoch 146, Train Loss 0.0505, Accuracy 97.73%; Length Accuracy 99.56%, Dups Accuracy 99.75% | Validation Loss 0.2530, Accuracy 86.57%; Length Accuracy 95.00%, Dups Accuracy 97.89%; taken 134 sec\n",
      "Epoch 147, Train Loss 0.0471, Accuracy 97.83%; Length Accuracy 99.64%, Dups Accuracy 99.82% | Validation Loss 0.2537, Accuracy 86.64%; Length Accuracy 95.26%, Dups Accuracy 98.37%; taken 134 sec\n",
      "Epoch 148, Train Loss 0.0510, Accuracy 97.62%; Length Accuracy 99.51%, Dups Accuracy 99.79% | Validation Loss 0.2543, Accuracy 85.53%; Length Accuracy 94.94%, Dups Accuracy 97.94%; taken 134 sec\n",
      "Epoch 149, Train Loss 0.0450, Accuracy 97.92%; Length Accuracy 99.60%, Dups Accuracy 99.79% | Validation Loss 0.2823, Accuracy 85.01%; Length Accuracy 94.30%, Dups Accuracy 97.93%; taken 134 sec\n",
      "Epoch 150, Train Loss 0.0486, Accuracy 97.85%; Length Accuracy 99.56%, Dups Accuracy 99.83% | Validation Loss 0.2471, Accuracy 85.34%; Length Accuracy 95.30%, Dups Accuracy 98.24%; taken 134 sec\n"
     ]
    }
   ],
   "source": [
    "while epoch_step2 < EPOCHS_STEP2:\n",
    "    step2(epoch_step2, EPOCHS_STEP2)\n",
    "    epoch_step2 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentiles(dataset):\n",
    "    all_accuracies = []\n",
    "    all_probs = []\n",
    "    for (glyph_tensor, target, length, total_dups, curr_dups) in dataset:\n",
    "        t_loss, accuracy, accuracy_length, accuracy_dups, all_accuracy, all_prob = validation_step2(glyph_tensor, target, length, total_dups, curr_dups)\n",
    "        all_accuracies.append(all_accuracy)\n",
    "        all_probs.append(all_prob)\n",
    "    all_accuracies = tf.concat(all_accuracies, axis = 0)\n",
    "    all_probs = tf.concat(all_probs, axis = 0)\n",
    "    correctness = tf.cast(all_accuracies, 'bool')\n",
    "    correct = tf.boolean_mask(all_probs, correctness)\n",
    "    wrong = tf.boolean_mask(all_probs, ~correctness)\n",
    "    return correct, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_val_percentile():\n",
    "    correct, wrong = percentiles(val_dataset)\n",
    "    target_percentiles = [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 98, 99]\n",
    "    correct_percentile = np.percentile(correct.numpy(), target_percentiles)\n",
    "    wrong_percentile = np.percentile(wrong.numpy(), target_percentiles)\n",
    "    return pd.DataFrame([correct_percentile, wrong_percentile], columns=target_percentiles, index=['Correct', 'Wrong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>70</th>\n",
       "      <th>80</th>\n",
       "      <th>90</th>\n",
       "      <th>95</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Correct</th>\n",
       "      <td>0.543906</td>\n",
       "      <td>0.625736</td>\n",
       "      <td>0.818510</td>\n",
       "      <td>0.946189</td>\n",
       "      <td>0.989020</td>\n",
       "      <td>0.996495</td>\n",
       "      <td>0.998436</td>\n",
       "      <td>0.999179</td>\n",
       "      <td>0.999570</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wrong</th>\n",
       "      <td>0.170655</td>\n",
       "      <td>0.208686</td>\n",
       "      <td>0.287825</td>\n",
       "      <td>0.383096</td>\n",
       "      <td>0.505361</td>\n",
       "      <td>0.578287</td>\n",
       "      <td>0.667883</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.846591</td>\n",
       "      <td>0.917368</td>\n",
       "      <td>0.965124</td>\n",
       "      <td>0.993017</td>\n",
       "      <td>0.997606</td>\n",
       "      <td>0.999490</td>\n",
       "      <td>0.999826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1         2         5         10        20        30        40  \\\n",
       "Correct  0.543906  0.625736  0.818510  0.946189  0.989020  0.996495  0.998436   \n",
       "Wrong    0.170655  0.208686  0.287825  0.383096  0.505361  0.578287  0.667883   \n",
       "\n",
       "               50        60        70        80        90        95        98  \\\n",
       "Correct  0.999179  0.999570  0.999780  0.999900  0.999960  0.999981  0.999991   \n",
       "Wrong    0.759542  0.846591  0.917368  0.965124  0.993017  0.997606  0.999490   \n",
       "\n",
       "               99  \n",
       "Correct  0.999995  \n",
       "Wrong    0.999826  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_val_percentile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cangjie = Cangjie(encoder, decoder, length_decoder, dup_decoder)\n",
    "cangjie.build(input_shape = (None, 64, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./Cangjie_Model/assets\n"
     ]
    }
   ],
   "source": [
    "cangjie.save('./Cangjie_Model/')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
