{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from fontTools.ttLib import TTFont\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = 28; EBD_DIM = 256; UNIT_DIM = 128; BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glyph(object):\n",
    "    # transform character to bitmap\n",
    "    def __init__(self, fonts, size=64):\n",
    "        # load fonts, size. We will use 2 fonts for all CJK characters, so keep 2 codepoint books.\n",
    "        self.codepoints = [set() for _ in fonts]\n",
    "        self.size = int(size * 0.8)\n",
    "        self.size_img = size\n",
    "        self.pad = (size - self.size) // 2\n",
    "        self.fonts = [ImageFont.truetype(f, self.size) for f in fonts]\n",
    "        # use a cache to reduce computation if duplicated characters encountered.\n",
    "        self.cache = {}\n",
    "        for cp, font in zip(self.codepoints, fonts):\n",
    "            font = TTFont(font)\n",
    "            # store codepoints in font cmap into self.codepoints\n",
    "            for cmap in font['cmap'].tables:\n",
    "                if not cmap.isUnicode():\n",
    "                    continue\n",
    "                for k in cmap.cmap:\n",
    "                    cp.add(k)\n",
    "    \n",
    "    def draw(self, ch):\n",
    "        if ch in self.cache:\n",
    "            return self.cache[ch]\n",
    "        # search among fonts, use the first found\n",
    "        exist = False\n",
    "        for i in range(len(self.codepoints)):\n",
    "            if ord(ch) in self.codepoints[i]:\n",
    "                font = self.fonts[i]\n",
    "                exist = True\n",
    "                break\n",
    "        if not exist:\n",
    "            return None\n",
    "\n",
    "        img = Image.new('L', (self.size_img, self.size_img), 0)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        (width, baseline), (offset_x, offset_y) = font.font.getsize(ch)\n",
    "        draw.text((self.pad - offset_x, self.pad - offset_y + 4), ch, font=font, fill=255, stroke_fill=255) \n",
    "        img_array = np.array(img.getdata(), dtype='float32').reshape((self.size_img, self.size_img)) / 255\n",
    "        self.cache[ch] = img_array\n",
    "\n",
    "        return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphbook = Glyph(['data/fonts/HanaMinA.otf', 'data/fonts/HanaMinB.otf'])\n",
    "\n",
    "def _mapping(item):\n",
    "    char, code, dup_total, dup_curr = item\n",
    "    glyph = glyphbook.draw(char)\n",
    "    if glyph is not None:\n",
    "        return glyph, code, dup_total, dup_curr\n",
    "\n",
    "def preprocess_chart(chart, cores=multiprocessing.cpu_count()):\n",
    "    glyphs = []; codes = []\n",
    "    dup_total = []; dup_curr = []\n",
    "    with multiprocessing.Pool(processes=cores) as pool:\n",
    "        for item in pool.map(_mapping, chart.values):\n",
    "            if item is not None:\n",
    "                glyphs.append(item[0])\n",
    "                codes.append(item[1])\n",
    "                dup_total.append(item[2])\n",
    "                dup_curr.append(item[3])\n",
    "    return np.expand_dims(np.array(glyphs), -1), np.array(codes), np.array(dup_total), np.array(dup_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(code_table):\n",
    "    # Cangjie code consists only of a-z, with maximum length of 5, minimum of 1\n",
    "    # start with 0, a-z are 1-26, end and padding are 27\n",
    "    tokens = np.expand_dims(np.zeros(code_table.shape, dtype='int64'), -1)\n",
    "    code_index = list(map(lambda x: list(map(lambda y: ord(y) - 96, list(x))) + [27] * (5-len(x)), code_table))\n",
    "    tokens = np.append(tokens, np.array(code_index), axis=-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chart = pd.read_csv('data/cangjie6.txt', delimiter='\\t', header=None, names=['Char', 'Code'], keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "for char, code in code_chart.values:\n",
    "    if char in count:\n",
    "        count[char].append(code)\n",
    "        count[char].sort(key=len)\n",
    "        count[char].sort(key=lambda x: (len(x), x))\n",
    "    else:\n",
    "        count[char] = [code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = code_chart.Code.map(len).max()\n",
    "MAX_DUP = max(map(lambda x: len(x), count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chart['DuplicateTotal'] = code_chart['Char'].map(count).map(len).copy()\n",
    "code_chart['DuplicateCurrent'] = code_chart.apply(lambda x: count[x['Char']].index(x['Code']) + 1, axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphs, codes, dups_total, dups_curr = preprocess_chart(code_chart)\n",
    "tokens = tokenizer(codes)\n",
    "lengths = np.array([len(list(filter(lambda i: i < VOCAB - 1 and i > 0, x))) for x in tokens])\n",
    "lengths = np.array([np.identity(MAX_LEN)[i-1] for i in lengths], dtype='int64')\n",
    "dups_total = np.array([np.identity(MAX_DUP)[i-1] for i in dups_total], dtype='int64')\n",
    "dups_curr = np.array([np.identity(MAX_DUP)[i-1] for i in dups_curr], dtype='int64')\n",
    "del code_chart, codes, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_glyphs, validation_glyphs,\n",
    " train_tokens, validation_tokens,\n",
    " train_lengths, validation_lengths,\n",
    " train_dups_total, validation_dups_total,\n",
    " train_dups_curr, validation_dups_curr) = train_test_split(\n",
    "    glyphs, tokens, lengths, dups_total, dups_curr, test_size=0.1, random_state=1225)\n",
    "del glyphs, tokens, lengths, dups_total, dups_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = len(train_glyphs) // BATCH_SIZE\n",
    "num_steps_val = len(validation_glyphs) // BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_glyphs, train_tokens, train_lengths, train_dups_total, train_dups_curr))\n",
    "dataset = dataset.shuffle(train_glyphs.shape[0]).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((validation_glyphs, validation_tokens, validation_lengths, validation_dups_total, validation_dups_curr))\n",
    "val_dataset = val_dataset.shuffle(validation_glyphs.shape[0]).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "del train_glyphs, validation_glyphs, train_tokens, validation_tokens, train_lengths, validation_lengths\n",
    "del train_dups_total, validation_dups_total, train_dups_curr, validation_dups_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res_CNN(tf.keras.Model):\n",
    "    def __init__(self, feature_dim, kernel_size):\n",
    "        super(Res_CNN, self).__init__()\n",
    "        self.cnn1 = tf.keras.layers.Convolution2D(feature_dim, kernel_size, padding='same')\n",
    "        self.cnn2 = tf.keras.layers.Convolution2D(feature_dim, kernel_size, padding='same')\n",
    "        self.cnn3 = tf.keras.layers.Convolution2D(feature_dim, kernel_size, padding='same')\n",
    "        self.norm = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x_identity = tf.identity(x)\n",
    "        x = self.cnn2(x)\n",
    "        x_identity2 = tf.identity(x)\n",
    "        x = self.cnn3(x + x_identity)\n",
    "        x = self.norm(x + x_identity2)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # This is essentially a CNN layer, \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.res_cnn1 = Res_CNN(embedding_dim // 16, (3, 3))\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.res_cnn2 = Res_CNN(embedding_dim // 4, (3, 3))\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.res_cnn3 = Res_CNN(embedding_dim, (3, 3))\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim, activation='relu')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # x shape after cnn1 == (batch_size, 64, 64, embedding_dim // 16)\n",
    "        x = self.res_cnn1(x)\n",
    "        # x shape after pool1 == (batch_size, 32, 32, embedding_dim // 16)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # x shape after cnn2 == (batch_size, 32, 32, embedding_dim // 4)\n",
    "        x = self.res_cnn2(x)\n",
    "        # x shape after pool2 == (batch_size, 16, 16, embedding_dim // 4)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # x shape after cnn3 == (batch_size, 16, 16, embedding_dim)\n",
    "        x = self.res_cnn3(x)\n",
    "        # reshape from (batch_size, 16, 16, embedding_dim) to (batch_size, 256, embedding_dim)\n",
    "        x = tf.reshape(x, [x.shape[0], -1, x.shape[-1]])\n",
    "        # x shape after fc == (batch_size, 256, embedding_dim)\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bahdanau_Attention(tf.keras.Model):\n",
    "    def __init__(self, attention_dim):\n",
    "        super(Bahdanau_Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(attention_dim)\n",
    "        self.W2 = tf.keras.layers.Dense(attention_dim)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_Encoder output) shape == (batch_size, 256, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 256, attention_dim)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 256, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, max_length, hidden_size, vocab_size):\n",
    "        super(Simple_Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = Bahdanau_Attention(hidden_size)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, feature, position):\n",
    "        # y shape (batch_size, hidden_size)\n",
    "        y = self.embedding(position)\n",
    "        # x shape (batch_size, embedding_dim)\n",
    "        x, w = self.attention(feature, y)\n",
    "        # x shape (batch_size, hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        # x shape (batch_size, vocab_size)\n",
    "        x = self.fc2(x)\n",
    "        return x, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length_Decoder(tf.keras.Model):\n",
    "    def __init__(self, max_length):\n",
    "        super(Length_Decoder, self).__init__()\n",
    "        self.pool = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.fc1 = tf.keras.layers.Dense(max_length * 16, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(max_length * 16, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(max_length * 4, activation='relu')\n",
    "        self.fc4 = tf.keras.layers.Dense(max_length)\n",
    "        \n",
    "    def call(self, x, d_t=None, d_c=None):\n",
    "        x = tf.reshape(x, (x.shape[0], 16, 16, x.shape[-1]))\n",
    "        x = self.pool(x) # shape = (batch_size, 8, 8, embedding_dim)\n",
    "        x = self.fc1(x)\n",
    "        x = tf.reshape(x, (x.shape[0], -1))\n",
    "        if d_t != None and d_c != None:\n",
    "            d = tf.concat([tf.cast(d_t, 'float32'), tf.cast(d_c, 'float32')], axis=-1)\n",
    "            x = tf.concat([d, x], axis=-1)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        # shape = (batch_size, max_length)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size, max_length):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru1 = tf.keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                        return_state=True, recurrent_initializer='glorot_uniform', dropout=0.3)\n",
    "        self.gru2 = tf.keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                        return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.gru3 = tf.keras.layers.GRU(self.hidden_size, return_sequences=True,\n",
    "                                        return_state=True, recurrent_initializer='glorot_uniform', dropout=0.3)\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = Bahdanau_Attention(hidden_size)\n",
    "\n",
    "    def call(self, x, l, d_t, d_c, features, hidden, training=True):\n",
    "        # x is forward direction, y is beckward direction\n",
    "        # defining attention as a separate model\n",
    "        l = tf.cast(l, 'float32')\n",
    "        hidden_0_with_length = tf.concat([l, hidden[0]], axis=-1)\n",
    "        context_vector, attention_weights = self.attention(features, hidden_0_with_length)\n",
    "        l = tf.expand_dims(l, 1)\n",
    "        d = tf.expand_dims(tf.concat([tf.cast(d_t, 'float32'), tf.cast(d_c, 'float32')], axis=-1), 1)\n",
    "\n",
    "        # x shape before is (batch_size, 1) since it is passed through one by one at a time\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # context_vector shape is (batch_size, embedding_dim)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # x shape is (batch_size, 1, hidden_size)\n",
    "        # state is new hidden used in next step\n",
    "        x, state1 = self.gru1(x, initial_state = hidden[0], training=training)\n",
    "        x_identity = tf.identity(x)\n",
    "        x = tf.concat([d, l, x], axis=-1)\n",
    "        x, state2 = self.gru2(x, initial_state = hidden[1], training=training)\n",
    "        x_identity2 = tf.identity(x)\n",
    "        x, state3 = self.gru3(x + x_identity, initial_state = hidden[2], training=training)\n",
    "        # x shape (batch_size, 1, max_length + hidden_size)\n",
    "        x = tf.concat([d, l, x + x_identity2], axis=-1)\n",
    "        x = tf.reshape(x, (x.shape[0], -1))\n",
    "        # x shape (batch_size, hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        # x shape (batch_size, vocab_size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, [state1, state2, state3], attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        # generate new hidden layer with different batch size\n",
    "        return [tf.zeros((batch_size, self.hidden_size)) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_step1 = tf.keras.optimizers.Adam()\n",
    "optimizer_step2 = tf.keras.optimizers.Adam()\n",
    "optimizer_length = tf.keras.optimizers.Adam()\n",
    "optimizer_dups = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    pred_index = tf.math.argmax(pred, axis=-1)\n",
    "    return tf.math.reduce_mean(tf.cast(pred_index == real, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step1(glyph, target, length, total_dups, curr_dups):\n",
    "    loss = 0; accuracy = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(glyph)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            position = tf.convert_to_tensor(np.repeat(i-1, target.shape[0]), dtype='int64')\n",
    "            prediction, weight = simple_decoder(features, position)\n",
    "            loss += loss_function(target[:, i], prediction)\n",
    "            accuracy += accuracy_function(target[:, i], prediction)\n",
    "\n",
    "    trainable_variables = simple_decoder.trainable_variables + encoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer_step1.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape_length:\n",
    "        length_pred = length_decoder(features, total_dups, curr_dups)\n",
    "        loss_length = loss_function(tf.math.argmax(length, axis=-1), length_pred)\n",
    "\n",
    "    gradients_length = tape_length.gradient(loss_length, length_decoder.trainable_variables)\n",
    "    optimizer_length.apply_gradients(zip(gradients_length, length_decoder.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape_dups:\n",
    "        dups_pred = dup_decoder(features)\n",
    "        loss_dups = loss_function(tf.math.argmax(total_dups, axis=-1), dups_pred)\n",
    "\n",
    "    gradients_dups = tape_dups.gradient(loss_dups, dup_decoder.trainable_variables)\n",
    "    optimizer_dups.apply_gradients(zip(gradients_dups, dup_decoder.trainable_variables))\n",
    "    \n",
    "    return loss / (target.shape[1] - 1), accuracy / (target.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step1(glyph, target):\n",
    "    loss = 0; accuracy = 0\n",
    "    feature = encoder(glyph, training=False)\n",
    "    for i in range(1, target.shape[1]):\n",
    "        position = tf.convert_to_tensor(np.repeat(i-1, target.shape[0]), dtype='int64')\n",
    "        prediction, weight = simple_decoder(feature, position)\n",
    "        loss += loss_function(target[:, i], prediction)\n",
    "        accuracy += accuracy_function(target[:, i], prediction)\n",
    "    return loss / (target.shape[1] - 1), accuracy / (target.shape[1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step1(epoch):\n",
    "    start = time.time()\n",
    "    total_loss = 0; val_loss = 0\n",
    "    total_accuracy = 0; val_accuracy = 0\n",
    "\n",
    "    for (batch, (glyph_tensor, target, length, total_dups, curr_dups)) in enumerate(dataset):\n",
    "        t_loss, accuracy = train_step1(glyph_tensor, target, length, total_dups, curr_dups)\n",
    "        total_loss += t_loss\n",
    "        total_accuracy += accuracy\n",
    "        print('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%}; progress {:.1%}, taken {:.0f} sec'.format(\n",
    "            epoch + 1, total_loss/batch, total_accuracy / batch, batch / num_steps, time.time() - start), end='\\r')\n",
    "    \n",
    "    for (glyph_tensor, target, _, _, _) in val_dataset:\n",
    "        t_loss, accuracy = validation_step1(glyph_tensor, target)\n",
    "        val_loss += t_loss\n",
    "        val_accuracy += accuracy\n",
    "   \n",
    "    # storing the epoch end loss value to plot later \n",
    "    ckpt_manager_step1.save()\n",
    "\n",
    "    print ('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%} | Validation Loss {:.4f}, Accuracy {:.2%}; taken {:.0f} sec'.format(\n",
    "        epoch+1, total_loss/num_steps, total_accuracy/num_steps, val_loss/num_steps_val, val_accuracy/num_steps_val, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, max_length, length, total_dups, curr_dups):\n",
    "    # start with 0\n",
    "    dec_input = tf.convert_to_tensor([[0]]*features.shape[0], dtype='int64')\n",
    "    hidden = decoder.reset_state(batch_size=features.shape[0])\n",
    "    probability = tf.convert_to_tensor([1]*features.shape[0], dtype='float32')\n",
    "    # iterate predictions, no teacher forcing here\n",
    "    for i in range(max_length):\n",
    "        prediction, hidden, attention_weights = decoder(\n",
    "            tf.expand_dims(dec_input[:, i], 1), length, total_dups, curr_dups, features, hidden, training=False)\n",
    "        # we need deterministic result\n",
    "        probability *= tf.math.reduce_max(tf.math.softmax(prediction, axis=-1), axis=-1)\n",
    "        predicted_id = tf.math.argmax(prediction, axis=-1)\n",
    "        dec_input = tf.concat([dec_input, tf.expand_dims(predicted_id, 1)], axis=1)\n",
    "    return dec_input, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(features, target, length, total_dups, curr_dups, training=True):\n",
    "    hidden = decoder.reset_state(batch_size=features.shape[0])\n",
    "    predictions = tf.constant(0, dtype='float32', shape=(features.shape[0], 1, VOCAB))\n",
    "    for i in range(target.shape[1]-1):\n",
    "        prediction, hidden, attention_weights = decoder(\n",
    "            tf.expand_dims(target[:, i], 1), length, total_dups, curr_dups, features, hidden, training=training)\n",
    "        predictions = tf.concat([predictions, tf.expand_dims(prediction, 1)], axis=1)\n",
    "    return predictions[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_step2(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    loss_ = tf.reduce_mean(loss_, axis=0)\n",
    "    return tf.reduce_sum(loss_)\n",
    "\n",
    "def accuracy_function_step2(real, pred):\n",
    "    accuracy = tf.math.reduce_all(pred == real, 1)\n",
    "    return tf.math.reduce_mean(tf.cast(accuracy, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step2(glyph_tensor, target, length, total_dups, curr_dups):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(glyph_tensor)\n",
    "        predictions = predict_next(features, target, length, total_dups, curr_dups)\n",
    "        loss = loss_function_step2(target[:, 1:], predictions)\n",
    "    \n",
    "    trainable_variables = decoder.trainable_variables + encoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer_step2.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape_length:\n",
    "        length_pred = length_decoder(features, d_t=total_dups, d_c=curr_dups)\n",
    "        loss_length = loss_function(tf.math.argmax(length, axis=-1), length_pred)\n",
    "\n",
    "    gradients_length = tape_length.gradient(loss_length, length_decoder.trainable_variables)\n",
    "    optimizer_length.apply_gradients(zip(gradients_length, length_decoder.trainable_variables))\n",
    "    \n",
    "    with tf.GradientTape() as tape_dups:\n",
    "        dups_pred = dup_decoder(features)\n",
    "        loss_dups = loss_function(tf.math.argmax(total_dups, axis=-1), dups_pred)\n",
    "\n",
    "    gradients_dups = tape_dups.gradient(loss_dups, dup_decoder.trainable_variables)\n",
    "    optimizer_dups.apply_gradients(zip(gradients_dups, dup_decoder.trainable_variables))\n",
    "    \n",
    "    # calculate accuracy based on the code's whole string\n",
    "    predictions_id, _ = predict(features, MAX_LEN, tf.nn.softmax(length_pred, axis=-1), total_dups, curr_dups)\n",
    "    accuracy = accuracy_function_step2(predictions_id, target)\n",
    "    accuracy_length = accuracy_function(tf.math.argmax(length, axis=-1), length_pred)\n",
    "\n",
    "    return loss / (target.shape[1] - 1), accuracy, loss_length, accuracy_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step2(glyph_tensor, target, length, total_dups, curr_dups):\n",
    "    features = encoder(glyph_tensor, training=False)\n",
    "    dups_pred = tf.nn.softmax(dup_decoder(features), axis=-1)\n",
    "    predictions = predict_next(features, target, length, dups_pred, curr_dups, training=False)\n",
    "    loss = loss_function_step2(target[:, 1:], predictions)\n",
    "    length_pred = length_decoder(features, d_t=dups_pred, d_c=curr_dups)\n",
    "\n",
    "    # calculate accuracy based on the code's whole string\n",
    "    predictions_id, _ = predict(features, MAX_LEN, tf.nn.softmax(length_pred, axis=-1), dups_pred, curr_dups)\n",
    "    accuracy = accuracy_function_step2(predictions_id, target)\n",
    "    loss_length = loss_function(tf.math.argmax(length, axis=-1), length_pred)\n",
    "    accuracy_length = accuracy_function(tf.math.argmax(length, axis=-1), length_pred)\n",
    "    \n",
    "    return loss / (target.shape[1] - 1), accuracy, loss_length, accuracy_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step2(epoch):\n",
    "    start = time.time()\n",
    "    total_loss = 0; val_loss = 0; len_loss = 0; val_len_loss = 0\n",
    "    total_accuracy = 0; val_accuracy = 0; len_accu = 0; val_len_accu = 0\n",
    "\n",
    "    for (batch, (glyph_tensor, target, length, total_dups, curr_dups)) in enumerate(dataset):\n",
    "#        if batch == 0:\n",
    "#            tf.summary.trace_on(graph=True, profiler=True)\n",
    "        t_loss, accuracy, loss_length, accuracy_length = train_step2(glyph_tensor, target, length, total_dups, curr_dups)\n",
    "#        if batch == 0:\n",
    "#            with graph_summary_writer.as_default():\n",
    "#                tf.summary.trace_export(name=\"train_trace\", step=epoch, profiler_outdir=graph_log_dir)\n",
    "#            tf.summary.trace_off()\n",
    "        total_loss += t_loss; total_accuracy += accuracy\n",
    "        len_loss += loss_length; len_accu += accuracy_length\n",
    "        \n",
    "        print('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%}; Length Loss {:.4f}, Accuracy {:.2%}; progress {:.1%}, taken {:.0f} sec'.format(\n",
    "            epoch + 1, total_loss/batch, total_accuracy/batch, len_loss/batch, len_accu/batch, batch/num_steps, time.time() - start), end='\\r')\n",
    "    \n",
    "    for (glyph_tensor, target, length, total_dups, curr_dups) in val_dataset:\n",
    "        t_loss, accuracy, loss_length, accuracy_length = validation_step2(glyph_tensor, target, length, total_dups, curr_dups)\n",
    "        val_loss += t_loss; val_accuracy += accuracy\n",
    "        val_len_loss += loss_length; val_len_accu += accuracy_length\n",
    "   \n",
    "    # storing the epoch end loss value to plot later\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', (total_loss / num_steps), step=epoch)\n",
    "        tf.summary.scalar('accuracy', (total_accuracy / num_steps), step=epoch)\n",
    "        tf.summary.scalar('length_loss', (len_loss / num_steps), step=epoch)\n",
    "        tf.summary.scalar('length_accuracy', (len_accu / num_steps), step=epoch)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_loss / num_steps_val, step=epoch)\n",
    "        tf.summary.scalar('accuracy', val_accuracy / num_steps_val, step=epoch)\n",
    "        tf.summary.scalar('length_loss', val_len_loss / num_steps_val, step=epoch)\n",
    "        tf.summary.scalar('length_accuracy', val_len_accu / num_steps_val, step=epoch)\n",
    "    \n",
    "    ckpt_manager_step2.save()\n",
    "\n",
    "    print('Epoch {}, Train Loss {:.4f}, Accuracy {:.2%}; Length Loss {:.4f}, Accuracy {:.2%} | Validation Loss {:.4f}, Accuracy {:.2%}, Length Loss {:.4f}, Accuracy {:.2%}, taken {:.0f} sec'.format(\n",
    "        epoch + 1, total_loss/num_steps, total_accuracy/num_steps, len_loss/num_steps, len_accu/num_steps,\n",
    "        val_loss/num_steps_val, val_accuracy/num_steps_val, val_len_loss/num_steps_val, val_len_accu/num_steps_val, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim = EBD_DIM)\n",
    "simple_decoder = Simple_Decoder(embedding_dim = EBD_DIM, max_length = MAX_LEN, hidden_size = UNIT_DIM, vocab_size = VOCAB)\n",
    "length_decoder = Length_Decoder(max_length = MAX_LEN)\n",
    "dup_decoder = Length_Decoder(max_length = MAX_DUP)\n",
    "decoder = RNN_Decoder(embedding_dim=EBD_DIM, hidden_size=UNIT_DIM, max_length = MAX_LEN, vocab_size=VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a checkpoint to store weights\n",
    "checkpoint_path_step1 = './checkpoints/train_step1'\n",
    "ckpt_step1 = tf.train.Checkpoint(encoder=encoder, decoder=simple_decoder, length_decoder=length_decoder,\n",
    "    dup_decoder=dup_decoder, optimizer=optimizer_step1, optimizer_length=optimizer_length, optimizer_dups=optimizer_dups)\n",
    "ckpt_manager_step1 = tf.train.CheckpointManager(ckpt_step1, checkpoint_path_step1, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a checkpoint to store weights\n",
    "checkpoint_path_step2 = \"./checkpoints/train_step2\"\n",
    "ckpt_step2 = tf.train.Checkpoint(encoder=encoder, decoder=decoder, length_decoder=length_decoder,\n",
    "    dup_decoder=dup_decoder, optimizer=optimizer_step2, optimizer_length=optimizer_length, optimizer_dups=optimizer_dups)\n",
    "ckpt_manager_step2 = tf.train.CheckpointManager(ckpt_step2, checkpoint_path_step2, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "#graph_log_dir = 'logs/gradient_tape/' + current_time + '/func'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "#graph_summary_writer = tf.summary.create_file_writer(graph_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_STEP1 = 20\n",
    "\n",
    "epoch_step1 = 0\n",
    "if ckpt_manager_step1.latest_checkpoint:\n",
    "    epoch_step1 = int(ckpt_manager_step1.latest_checkpoint.split('-')[-1])\n",
    "    ckpt_step1.restore(ckpt_manager_step1.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss 2.7853, Accuracy 22.87% | Validation Loss 2.2880, Accuracy 38.07%; taken 94 sec\n",
      "Epoch 2, Train Loss 1.7475, Accuracy 49.98% | Validation Loss 1.3232, Accuracy 63.96%; taken 77 sec\n",
      "Epoch 3, Train Loss 1.1994, Accuracy 64.86% | Validation Loss 1.0355, Accuracy 71.96%; taken 77 sec\n",
      "Epoch 4, Train Loss 0.9565, Accuracy 71.67% | Validation Loss 0.8395, Accuracy 77.50%; taken 77 sec\n",
      "Epoch 5, Train Loss 0.8159, Accuracy 75.67% | Validation Loss 0.7274, Accuracy 80.99%; taken 77 sec\n",
      "Epoch 6, Train Loss 0.7240, Accuracy 78.27% | Validation Loss 0.6424, Accuracy 83.46%; taken 76 sec\n",
      "Epoch 7, Train Loss 0.6610, Accuracy 80.03% | Validation Loss 0.6226, Accuracy 83.98%; taken 77 sec\n",
      "Epoch 8, Train Loss 0.6121, Accuracy 81.45% | Validation Loss 0.5893, Accuracy 84.85%; taken 77 sec\n",
      "Epoch 9, Train Loss 0.5737, Accuracy 82.57% | Validation Loss 0.5374, Accuracy 86.54%; taken 77 sec\n",
      "Epoch 10, Train Loss 0.5438, Accuracy 83.43% | Validation Loss 0.5150, Accuracy 86.88%; taken 77 sec\n",
      "Epoch 11, Train Loss 0.5174, Accuracy 84.21% | Validation Loss 0.5146, Accuracy 87.19%; taken 77 sec\n",
      "Epoch 12, Train Loss 0.4958, Accuracy 84.82% | Validation Loss 0.4892, Accuracy 87.87%; taken 77 sec\n",
      "Epoch 13, Train Loss 0.4741, Accuracy 85.46% | Validation Loss 0.4682, Accuracy 88.16%; taken 76 sec\n",
      "Epoch 14, Train Loss 0.4602, Accuracy 85.82% | Validation Loss 0.4634, Accuracy 88.39%; taken 77 sec\n",
      "Epoch 15, Train Loss 0.4456, Accuracy 86.29% | Validation Loss 0.4598, Accuracy 88.87%; taken 77 sec\n",
      "Epoch 16, Train Loss 0.4331, Accuracy 86.63% | Validation Loss 0.4435, Accuracy 89.04%; taken 77 sec\n",
      "Epoch 17, Train Loss 0.4227, Accuracy 86.88% | Validation Loss 0.4327, Accuracy 89.54%; taken 77 sec\n",
      "Epoch 18, Train Loss 0.4093, Accuracy 87.31% | Validation Loss 0.4224, Accuracy 89.67%; taken 77 sec\n",
      "Epoch 19, Train Loss 0.4013, Accuracy 87.56% | Validation Loss 0.4232, Accuracy 89.94%; taken 77 sec\n",
      "Epoch 20, Train Loss 0.3933, Accuracy 87.78% | Validation Loss 0.4146, Accuracy 89.98%; taken 77 sec\n"
     ]
    }
   ],
   "source": [
    "while epoch_step1 < EPOCHS_STEP1:\n",
    "    step1(epoch_step1)\n",
    "    epoch_step1 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_STEP2 = 120\n",
    "\n",
    "epoch_step2 = 0\n",
    "if ckpt_manager_step2.latest_checkpoint:\n",
    "    epoch_step2 = int(ckpt_manager_step2.latest_checkpoint.split('-')[-1])\n",
    "    ckpt_step2.restore(ckpt_manager_step2.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss 1.4670, Accuracy 17.85%; Length Loss 0.3017, Accuracy 88.40% | Validation Loss 0.5027, Accuracy 47.13%, Length Loss 0.2537, Accuracy 93.30%, taken 124 sec\n",
      "Epoch 2, Train Loss 0.5261, Accuracy 52.17%; Length Loss 0.2093, Accuracy 92.06% | Validation Loss 0.3629, Accuracy 59.45%, Length Loss 0.2466, Accuracy 93.17%, taken 88 sec\n",
      "Epoch 3, Train Loss 0.4129, Accuracy 60.40%; Length Loss 0.2019, Accuracy 92.43% | Validation Loss 0.3100, Accuracy 64.80%, Length Loss 0.2264, Accuracy 93.95%, taken 88 sec\n",
      "Epoch 4, Train Loss 0.3576, Accuracy 64.60%; Length Loss 0.1928, Accuracy 92.74% | Validation Loss 0.2902, Accuracy 67.15%, Length Loss 0.2223, Accuracy 94.40%, taken 88 sec\n",
      "Epoch 5, Train Loss 0.3196, Accuracy 67.83%; Length Loss 0.1822, Accuracy 93.24% | Validation Loss 0.2591, Accuracy 69.32%, Length Loss 0.2420, Accuracy 93.36%, taken 88 sec\n",
      "Epoch 6, Train Loss 0.2956, Accuracy 70.06%; Length Loss 0.1793, Accuracy 93.36% | Validation Loss 0.2339, Accuracy 72.41%, Length Loss 0.2103, Accuracy 94.95%, taken 88 sec\n",
      "Epoch 7, Train Loss 0.2775, Accuracy 71.55%; Length Loss 0.1791, Accuracy 93.36% | Validation Loss 0.2239, Accuracy 74.03%, Length Loss 0.2059, Accuracy 95.04%, taken 88 sec\n",
      "Epoch 8, Train Loss 0.2586, Accuracy 73.26%; Length Loss 0.1743, Accuracy 93.54% | Validation Loss 0.2204, Accuracy 73.92%, Length Loss 0.1961, Accuracy 95.32%, taken 87 sec\n",
      "Epoch 9, Train Loss 0.2470, Accuracy 74.23%; Length Loss 0.1700, Accuracy 93.69% | Validation Loss 0.2083, Accuracy 75.36%, Length Loss 0.2098, Accuracy 94.91%, taken 88 sec\n",
      "Epoch 10, Train Loss 0.2337, Accuracy 75.63%; Length Loss 0.1625, Accuracy 94.05% | Validation Loss 0.1965, Accuracy 77.46%, Length Loss 0.1940, Accuracy 95.46%, taken 87 sec\n",
      "Epoch 11, Train Loss 0.2243, Accuracy 76.50%; Length Loss 0.1585, Accuracy 94.19% | Validation Loss 0.1975, Accuracy 77.10%, Length Loss 0.2065, Accuracy 95.39%, taken 88 sec\n",
      "Epoch 12, Train Loss 0.2145, Accuracy 77.70%; Length Loss 0.1532, Accuracy 94.39% | Validation Loss 0.1900, Accuracy 75.79%, Length Loss 0.2585, Accuracy 92.43%, taken 87 sec\n",
      "Epoch 13, Train Loss 0.2090, Accuracy 78.25%; Length Loss 0.1470, Accuracy 94.61% | Validation Loss 0.1899, Accuracy 77.81%, Length Loss 0.1987, Accuracy 95.46%, taken 88 sec\n",
      "Epoch 14, Train Loss 0.1996, Accuracy 78.98%; Length Loss 0.1486, Accuracy 94.57% | Validation Loss 0.1777, Accuracy 79.16%, Length Loss 0.1901, Accuracy 95.87%, taken 89 sec\n",
      "Epoch 15, Train Loss 0.1941, Accuracy 79.64%; Length Loss 0.1400, Accuracy 94.90% | Validation Loss 0.1757, Accuracy 78.78%, Length Loss 0.2336, Accuracy 93.86%, taken 89 sec\n",
      "Epoch 16, Train Loss 0.1895, Accuracy 80.26%; Length Loss 0.1412, Accuracy 94.91% | Validation Loss 0.1716, Accuracy 80.55%, Length Loss 0.1936, Accuracy 95.59%, taken 89 sec\n",
      "Epoch 17, Train Loss 0.1809, Accuracy 80.87%; Length Loss 0.1389, Accuracy 94.95% | Validation Loss 0.1779, Accuracy 79.43%, Length Loss 0.1992, Accuracy 95.15%, taken 89 sec\n",
      "Epoch 19, Train Loss 0.1751, Accuracy 81.47%; Length Loss 0.1355, Accuracy 94.99% | Validation Loss 0.1673, Accuracy 81.14%, Length Loss 0.1883, Accuracy 95.81%, taken 88 sec\n",
      "Epoch 20, Train Loss 0.1692, Accuracy 82.10%; Length Loss 0.1328, Accuracy 95.21% | Validation Loss 0.1717, Accuracy 80.62%, Length Loss 0.1959, Accuracy 95.79%, taken 88 sec\n",
      "Epoch 21, Train Loss 0.1647, Accuracy 82.60%; Length Loss 0.1291, Accuracy 95.38% | Validation Loss 0.1622, Accuracy 81.38%, Length Loss 0.1850, Accuracy 96.02%, taken 88 sec\n",
      "Epoch 22, Train Loss 0.1604, Accuracy 83.10%; Length Loss 0.1293, Accuracy 95.37% | Validation Loss 0.1631, Accuracy 81.84%, Length Loss 0.1735, Accuracy 96.08%, taken 88 sec\n",
      "Epoch 23, Train Loss 0.1578, Accuracy 83.29%; Length Loss 0.1260, Accuracy 95.32% | Validation Loss 0.1620, Accuracy 80.21%, Length Loss 0.2303, Accuracy 94.40%, taken 88 sec\n",
      "Epoch 24, Train Loss 0.1525, Accuracy 83.81%; Length Loss 0.1252, Accuracy 95.55% | Validation Loss 0.1668, Accuracy 81.33%, Length Loss 0.1909, Accuracy 95.67%, taken 88 sec\n",
      "Epoch 25, Train Loss 0.1526, Accuracy 83.99%; Length Loss 0.1217, Accuracy 95.66% | Validation Loss 0.1581, Accuracy 82.30%, Length Loss 0.1886, Accuracy 96.08%, taken 88 sec\n",
      "Epoch 26, Train Loss 0.1494, Accuracy 84.09%; Length Loss 0.1252, Accuracy 95.49% | Validation Loss 0.1536, Accuracy 81.44%, Length Loss 0.2227, Accuracy 94.47%, taken 88 sec\n",
      "Epoch 27, Train Loss 0.1475, Accuracy 84.42%; Length Loss 0.1209, Accuracy 95.59% | Validation Loss 0.1521, Accuracy 82.61%, Length Loss 0.1810, Accuracy 96.02%, taken 88 sec\n",
      "Epoch 28, Train Loss 0.1429, Accuracy 84.83%; Length Loss 0.1200, Accuracy 95.71% | Validation Loss 0.1567, Accuracy 82.73%, Length Loss 0.1720, Accuracy 96.71%, taken 88 sec\n",
      "Epoch 29, Train Loss 0.1385, Accuracy 85.38%; Length Loss 0.1143, Accuracy 95.90% | Validation Loss 0.1562, Accuracy 82.69%, Length Loss 0.1755, Accuracy 96.38%, taken 87 sec\n",
      "Epoch 30, Train Loss 0.1374, Accuracy 85.51%; Length Loss 0.1141, Accuracy 95.97% | Validation Loss 0.1504, Accuracy 82.95%, Length Loss 0.1721, Accuracy 96.52%, taken 87 sec\n",
      "Epoch 31, Train Loss 0.1343, Accuracy 85.93%; Length Loss 0.1103, Accuracy 96.10% | Validation Loss 0.1523, Accuracy 81.62%, Length Loss 0.2095, Accuracy 95.11%, taken 88 sec\n",
      "Epoch 32, Train Loss 0.1340, Accuracy 85.89%; Length Loss 0.1108, Accuracy 96.07% | Validation Loss 0.1459, Accuracy 82.77%, Length Loss 0.2032, Accuracy 95.53%, taken 87 sec\n",
      "Epoch 33, Train Loss 0.1313, Accuracy 86.10%; Length Loss 0.1119, Accuracy 95.95% | Validation Loss 0.1431, Accuracy 83.35%, Length Loss 0.2103, Accuracy 95.30%, taken 87 sec\n",
      "Epoch 34, Train Loss 0.1290, Accuracy 86.43%; Length Loss 0.1086, Accuracy 96.10% | Validation Loss 0.1420, Accuracy 84.26%, Length Loss 0.1738, Accuracy 96.54%, taken 87 sec\n",
      "Epoch 35, Train Loss 0.1278, Accuracy 86.40%; Length Loss 0.1111, Accuracy 95.98% | Validation Loss 0.1408, Accuracy 84.10%, Length Loss 0.1666, Accuracy 96.84%, taken 87 sec\n",
      "Epoch 36, Train Loss 0.1260, Accuracy 86.79%; Length Loss 0.1103, Accuracy 96.11% | Validation Loss 0.1436, Accuracy 83.30%, Length Loss 0.1814, Accuracy 96.36%, taken 88 sec\n",
      "Epoch 37, Train Loss 0.1237, Accuracy 87.15%; Length Loss 0.1049, Accuracy 96.29% | Validation Loss 0.1463, Accuracy 83.51%, Length Loss 0.1900, Accuracy 96.18%, taken 88 sec\n",
      "Epoch 38, Train Loss 0.1212, Accuracy 87.16%; Length Loss 0.1052, Accuracy 96.30% | Validation Loss 0.1444, Accuracy 83.71%, Length Loss 0.1723, Accuracy 96.38%, taken 88 sec\n",
      "Epoch 39, Train Loss 0.1182, Accuracy 87.56%; Length Loss 0.1025, Accuracy 96.44% | Validation Loss 0.1438, Accuracy 83.87%, Length Loss 0.1972, Accuracy 95.67%, taken 88 sec\n",
      "Epoch 40, Train Loss 0.1189, Accuracy 87.51%; Length Loss 0.1006, Accuracy 96.44% | Validation Loss 0.1409, Accuracy 83.50%, Length Loss 0.2265, Accuracy 94.86%, taken 88 sec\n",
      "Epoch 41, Train Loss 0.1165, Accuracy 87.69%; Length Loss 0.1005, Accuracy 96.47% | Validation Loss 0.1416, Accuracy 84.06%, Length Loss 0.1867, Accuracy 95.99%, taken 88 sec\n",
      "Epoch 42, Train Loss 0.1150, Accuracy 87.94%; Length Loss 0.0978, Accuracy 96.56% | Validation Loss 0.1403, Accuracy 84.81%, Length Loss 0.1795, Accuracy 96.35%, taken 87 sec\n",
      "Epoch 43, Train Loss 0.1124, Accuracy 87.99%; Length Loss 0.0991, Accuracy 96.44% | Validation Loss 0.1428, Accuracy 83.99%, Length Loss 0.1964, Accuracy 95.62%, taken 88 sec\n",
      "Epoch 44, Train Loss 0.1123, Accuracy 88.09%; Length Loss 0.1011, Accuracy 96.44% | Validation Loss 0.1428, Accuracy 84.44%, Length Loss 0.1737, Accuracy 96.47%, taken 87 sec\n",
      "Epoch 45, Train Loss 0.1102, Accuracy 88.37%; Length Loss 0.0993, Accuracy 96.47% | Validation Loss 0.1421, Accuracy 83.92%, Length Loss 0.2033, Accuracy 95.42%, taken 88 sec\n",
      "Epoch 46, Train Loss 0.1096, Accuracy 88.59%; Length Loss 0.0947, Accuracy 96.68% | Validation Loss 0.1428, Accuracy 84.18%, Length Loss 0.1745, Accuracy 96.26%, taken 88 sec\n",
      "Epoch 47, Train Loss 0.1088, Accuracy 88.51%; Length Loss 0.0951, Accuracy 96.69% | Validation Loss 0.1419, Accuracy 83.99%, Length Loss 0.1770, Accuracy 96.28%, taken 87 sec\n",
      "Epoch 49, Train Loss 0.1039, Accuracy 89.18%; Length Loss 0.0899, Accuracy 96.92% | Validation Loss 0.1396, Accuracy 84.73%, Length Loss 0.1861, Accuracy 96.16%, taken 87 sec\n",
      "Epoch 50, Train Loss 0.1052, Accuracy 88.89%; Length Loss 0.0942, Accuracy 96.78% | Validation Loss 0.1363, Accuracy 85.44%, Length Loss 0.1813, Accuracy 96.36%, taken 88 sec\n",
      "Epoch 51, Train Loss 0.1014, Accuracy 89.44%; Length Loss 0.0885, Accuracy 96.94% | Validation Loss 0.1348, Accuracy 85.38%, Length Loss 0.1723, Accuracy 96.71%, taken 88 sec\n",
      "Epoch 52, Train Loss 0.1026, Accuracy 89.25%; Length Loss 0.0912, Accuracy 96.83% | Validation Loss 0.1401, Accuracy 85.17%, Length Loss 0.1798, Accuracy 96.59%, taken 87 sec\n",
      "Epoch 54, Train Loss 0.0987, Accuracy 89.72%; Length Loss 0.0865, Accuracy 97.04% | Validation Loss 0.1387, Accuracy 85.02%, Length Loss 0.1744, Accuracy 96.32%, taken 87 sec\n",
      "Epoch 55, Train Loss 0.1004, Accuracy 89.61%; Length Loss 0.0875, Accuracy 96.91% | Validation Loss 0.1369, Accuracy 85.44%, Length Loss 0.1767, Accuracy 96.78%, taken 87 sec\n",
      "Epoch 56, Train Loss 0.0970, Accuracy 90.03%; Length Loss 0.0821, Accuracy 97.18% | Validation Loss 0.1358, Accuracy 85.57%, Length Loss 0.1822, Accuracy 96.60%, taken 88 sec\n",
      "Epoch 57, Train Loss 0.0952, Accuracy 90.26%; Length Loss 0.0842, Accuracy 97.16% | Validation Loss 0.1303, Accuracy 86.09%, Length Loss 0.1677, Accuracy 96.90%, taken 88 sec\n",
      "Epoch 59, Train Loss 0.0939, Accuracy 90.28%; Length Loss 0.0813, Accuracy 97.15% | Validation Loss 0.1357, Accuracy 85.14%, Length Loss 0.2244, Accuracy 95.22%, taken 88 sec\n",
      "Epoch 60, Train Loss 0.0924, Accuracy 90.28%; Length Loss 0.0842, Accuracy 97.10% | Validation Loss 0.1321, Accuracy 86.22%, Length Loss 0.1783, Accuracy 96.78%, taken 88 sec\n",
      "Epoch 61, Train Loss 0.0913, Accuracy 90.59%; Length Loss 0.0801, Accuracy 97.24% | Validation Loss 0.1361, Accuracy 85.74%, Length Loss 0.1671, Accuracy 96.71%, taken 88 sec\n",
      "Epoch 62, Train Loss 0.0906, Accuracy 90.64%; Length Loss 0.0823, Accuracy 97.20% | Validation Loss 0.1370, Accuracy 84.47%, Length Loss 0.2028, Accuracy 95.63%, taken 87 sec\n",
      "Epoch 63, Train Loss 0.0929, Accuracy 90.61%; Length Loss 0.0807, Accuracy 97.21% | Validation Loss 0.1364, Accuracy 85.88%, Length Loss 0.1855, Accuracy 96.48%, taken 88 sec\n",
      "Epoch 64, Train Loss 0.0880, Accuracy 90.88%; Length Loss 0.0800, Accuracy 97.27% | Validation Loss 0.1364, Accuracy 86.09%, Length Loss 0.1722, Accuracy 97.04%, taken 87 sec\n",
      "Epoch 65, Train Loss 0.0879, Accuracy 90.90%; Length Loss 0.0764, Accuracy 97.35% | Validation Loss 0.1300, Accuracy 85.32%, Length Loss 0.2136, Accuracy 95.59%, taken 88 sec\n",
      "Epoch 66, Train Loss 0.0872, Accuracy 91.08%; Length Loss 0.0766, Accuracy 97.35% | Validation Loss 0.1378, Accuracy 86.08%, Length Loss 0.1683, Accuracy 97.17%, taken 87 sec\n",
      "Epoch 67, Train Loss 0.0855, Accuracy 91.28%; Length Loss 0.0764, Accuracy 97.38% | Validation Loss 0.1344, Accuracy 85.21%, Length Loss 0.1951, Accuracy 96.00%, taken 88 sec\n",
      "Epoch 68, Train Loss 0.0855, Accuracy 91.30%; Length Loss 0.0759, Accuracy 97.50% | Validation Loss 0.1332, Accuracy 86.48%, Length Loss 0.1888, Accuracy 96.62%, taken 87 sec\n",
      "Epoch 69, Train Loss 0.0843, Accuracy 91.54%; Length Loss 0.0741, Accuracy 97.49% | Validation Loss 0.1321, Accuracy 86.14%, Length Loss 0.1916, Accuracy 96.32%, taken 87 sec\n",
      "Epoch 70, Train Loss 0.0863, Accuracy 91.26%; Length Loss 0.0777, Accuracy 97.37% | Validation Loss 0.1334, Accuracy 86.68%, Length Loss 0.1676, Accuracy 96.89%, taken 88 sec\n",
      "Epoch 71, Train Loss 0.0848, Accuracy 91.43%; Length Loss 0.0761, Accuracy 97.40% | Validation Loss 0.1350, Accuracy 86.41%, Length Loss 0.1814, Accuracy 96.46%, taken 87 sec\n",
      "Epoch 72, Train Loss 0.0833, Accuracy 91.58%; Length Loss 0.0728, Accuracy 97.52% | Validation Loss 0.1388, Accuracy 84.88%, Length Loss 0.1958, Accuracy 96.13%, taken 88 sec\n",
      "Epoch 73, Train Loss 0.0818, Accuracy 91.74%; Length Loss 0.0713, Accuracy 97.66% | Validation Loss 0.1370, Accuracy 85.95%, Length Loss 0.1880, Accuracy 96.41%, taken 87 sec\n",
      "Epoch 74, Train Loss 0.0808, Accuracy 91.90%; Length Loss 0.0717, Accuracy 97.54% | Validation Loss 0.1357, Accuracy 85.98%, Length Loss 0.1688, Accuracy 96.94%, taken 88 sec\n",
      "Epoch 75, Train Loss 0.0791, Accuracy 91.99%; Length Loss 0.0694, Accuracy 97.67% | Validation Loss 0.1338, Accuracy 85.68%, Length Loss 0.1978, Accuracy 96.17%, taken 87 sec\n",
      "Epoch 76, Train Loss 0.0785, Accuracy 92.09%; Length Loss 0.0702, Accuracy 97.62% | Validation Loss 0.1299, Accuracy 85.70%, Length Loss 0.2029, Accuracy 96.13%, taken 88 sec\n",
      "Epoch 77, Train Loss 0.0794, Accuracy 92.00%; Length Loss 0.0681, Accuracy 97.75% | Validation Loss 0.1297, Accuracy 86.21%, Length Loss 0.1978, Accuracy 96.29%, taken 87 sec\n",
      "Epoch 78, Train Loss 0.0785, Accuracy 92.06%; Length Loss 0.0677, Accuracy 97.72% | Validation Loss 0.1416, Accuracy 85.97%, Length Loss 0.1719, Accuracy 97.08%, taken 88 sec\n",
      "Epoch 79, Train Loss 0.0780, Accuracy 92.25%; Length Loss 0.0692, Accuracy 97.64% | Validation Loss 0.1397, Accuracy 86.01%, Length Loss 0.1712, Accuracy 97.06%, taken 88 sec\n",
      "Epoch 80, Train Loss 0.0765, Accuracy 92.25%; Length Loss 0.0683, Accuracy 97.70% | Validation Loss 0.1317, Accuracy 87.13%, Length Loss 0.1785, Accuracy 97.09%, taken 88 sec\n",
      "Epoch 81, Train Loss 0.0767, Accuracy 92.14%; Length Loss 0.0704, Accuracy 97.63% | Validation Loss 0.1372, Accuracy 86.13%, Length Loss 0.1753, Accuracy 96.81%, taken 88 sec\n",
      "Epoch 82, Train Loss 0.0731, Accuracy 92.73%; Length Loss 0.0648, Accuracy 97.83% | Validation Loss 0.1324, Accuracy 86.85%, Length Loss 0.1660, Accuracy 97.24%, taken 88 sec\n",
      "Epoch 83, Train Loss 0.0735, Accuracy 92.65%; Length Loss 0.0660, Accuracy 97.87% | Validation Loss 0.1343, Accuracy 86.71%, Length Loss 0.1744, Accuracy 97.10%, taken 88 sec\n",
      "Epoch 84, Train Loss 0.0732, Accuracy 92.82%; Length Loss 0.0630, Accuracy 97.93% | Validation Loss 0.1341, Accuracy 86.70%, Length Loss 0.1681, Accuracy 97.36%, taken 88 sec\n",
      "Epoch 85, Train Loss 0.0726, Accuracy 92.74%; Length Loss 0.0671, Accuracy 97.79% | Validation Loss 0.1345, Accuracy 86.97%, Length Loss 0.1768, Accuracy 96.81%, taken 88 sec\n",
      "Epoch 86, Train Loss 0.0743, Accuracy 92.82%; Length Loss 0.0632, Accuracy 97.93% | Validation Loss 0.1326, Accuracy 87.14%, Length Loss 0.1698, Accuracy 97.31%, taken 88 sec\n",
      "Epoch 87, Train Loss 0.0706, Accuracy 93.06%; Length Loss 0.0609, Accuracy 98.00% | Validation Loss 0.1366, Accuracy 86.71%, Length Loss 0.1866, Accuracy 96.85%, taken 88 sec\n",
      "Epoch 88, Train Loss 0.0729, Accuracy 92.83%; Length Loss 0.0622, Accuracy 97.93% | Validation Loss 0.1326, Accuracy 86.63%, Length Loss 0.1916, Accuracy 96.42%, taken 88 sec\n",
      "Epoch 89, Train Loss 0.0712, Accuracy 93.06%; Length Loss 0.0630, Accuracy 97.93% | Validation Loss 0.1330, Accuracy 86.22%, Length Loss 0.1869, Accuracy 96.54%, taken 88 sec\n",
      "Epoch 91, Train Loss 0.0695, Accuracy 93.19%; Length Loss 0.0616, Accuracy 97.97% | Validation Loss 0.1362, Accuracy 86.61%, Length Loss 0.1995, Accuracy 96.72%, taken 87 sec\n",
      "Epoch 92, Train Loss 0.0683, Accuracy 93.33%; Length Loss 0.0610, Accuracy 97.96% | Validation Loss 0.1335, Accuracy 87.29%, Length Loss 0.1690, Accuracy 97.59%, taken 88 sec\n",
      "Epoch 93, Train Loss 0.0685, Accuracy 93.31%; Length Loss 0.0608, Accuracy 98.04% | Validation Loss 0.1360, Accuracy 86.79%, Length Loss 0.1790, Accuracy 97.06%, taken 88 sec\n",
      "Epoch 94, Train Loss 0.0679, Accuracy 93.32%; Length Loss 0.0615, Accuracy 98.00% | Validation Loss 0.1357, Accuracy 87.31%, Length Loss 0.1767, Accuracy 97.24%, taken 87 sec\n",
      "Epoch 95, Train Loss 0.0687, Accuracy 93.27%; Length Loss 0.0606, Accuracy 98.01% | Validation Loss 0.1336, Accuracy 87.35%, Length Loss 0.1814, Accuracy 97.24%, taken 88 sec\n",
      "Epoch 96, Train Loss 0.0662, Accuracy 93.65%; Length Loss 0.0582, Accuracy 98.17% | Validation Loss 0.1384, Accuracy 86.53%, Length Loss 0.1983, Accuracy 96.80%, taken 88 sec\n",
      "Epoch 97, Train Loss 0.0659, Accuracy 93.68%; Length Loss 0.0574, Accuracy 98.13% | Validation Loss 0.1419, Accuracy 85.66%, Length Loss 0.2203, Accuracy 95.51%, taken 87 sec\n",
      "Epoch 98, Train Loss 0.0648, Accuracy 93.63%; Length Loss 0.0587, Accuracy 98.09% | Validation Loss 0.1305, Accuracy 87.80%, Length Loss 0.1671, Accuracy 97.60%, taken 88 sec\n",
      "Epoch 99, Train Loss 0.0658, Accuracy 93.89%; Length Loss 0.0539, Accuracy 98.28% | Validation Loss 0.1405, Accuracy 86.65%, Length Loss 0.1751, Accuracy 97.44%, taken 88 sec\n",
      "Epoch 100, Train Loss 0.0662, Accuracy 93.68%; Length Loss 0.0567, Accuracy 98.15% | Validation Loss 0.1345, Accuracy 87.52%, Length Loss 0.1806, Accuracy 97.30%, taken 88 sec\n",
      "Epoch 101, Train Loss 0.0636, Accuracy 93.90%; Length Loss 0.0569, Accuracy 98.20% | Validation Loss 0.1334, Accuracy 87.59%, Length Loss 0.1796, Accuracy 97.59%, taken 88 sec\n",
      "Epoch 102, Train Loss 0.0636, Accuracy 93.82%; Length Loss 0.0565, Accuracy 98.17% | Validation Loss 0.1356, Accuracy 86.23%, Length Loss 0.2272, Accuracy 95.94%, taken 88 sec\n",
      "Epoch 103, Train Loss 0.0637, Accuracy 93.97%; Length Loss 0.0563, Accuracy 98.18% | Validation Loss 0.1320, Accuracy 87.73%, Length Loss 0.1749, Accuracy 97.26%, taken 87 sec\n",
      "Epoch 104, Train Loss 0.0630, Accuracy 94.01%; Length Loss 0.0549, Accuracy 98.25% | Validation Loss 0.1385, Accuracy 87.06%, Length Loss 0.1895, Accuracy 96.84%, taken 88 sec\n",
      "Epoch 105, Train Loss 0.0614, Accuracy 94.11%; Length Loss 0.0550, Accuracy 98.20% | Validation Loss 0.1370, Accuracy 87.29%, Length Loss 0.1703, Accuracy 97.32%, taken 88 sec\n",
      "Epoch 106, Train Loss 0.0627, Accuracy 93.96%; Length Loss 0.0552, Accuracy 98.19% | Validation Loss 0.1347, Accuracy 87.03%, Length Loss 0.2023, Accuracy 96.52%, taken 88 sec\n",
      "Epoch 107, Train Loss 0.0619, Accuracy 94.17%; Length Loss 0.0539, Accuracy 98.29% | Validation Loss 0.1366, Accuracy 87.15%, Length Loss 0.1912, Accuracy 96.90%, taken 87 sec\n",
      "Epoch 108, Train Loss 0.0627, Accuracy 94.05%; Length Loss 0.0534, Accuracy 98.31% | Validation Loss 0.1335, Accuracy 87.66%, Length Loss 0.1723, Accuracy 97.38%, taken 88 sec\n",
      "Epoch 109, Train Loss 0.0624, Accuracy 94.04%; Length Loss 0.0546, Accuracy 98.27% | Validation Loss 0.1366, Accuracy 86.08%, Length Loss 0.2293, Accuracy 95.30%, taken 88 sec\n",
      "Epoch 110, Train Loss 0.0613, Accuracy 94.14%; Length Loss 0.0559, Accuracy 98.18% | Validation Loss 0.1346, Accuracy 86.99%, Length Loss 0.1944, Accuracy 96.65%, taken 87 sec\n",
      "Epoch 111, Train Loss 0.0596, Accuracy 94.36%; Length Loss 0.0523, Accuracy 98.30% | Validation Loss 0.1375, Accuracy 86.79%, Length Loss 0.1957, Accuracy 96.85%, taken 88 sec\n",
      "Epoch 112, Train Loss 0.0615, Accuracy 94.27%; Length Loss 0.0561, Accuracy 98.22% | Validation Loss 0.1390, Accuracy 87.08%, Length Loss 0.1737, Accuracy 97.21%, taken 88 sec\n",
      "Epoch 113, Train Loss 0.0600, Accuracy 94.39%; Length Loss 0.0504, Accuracy 98.40% | Validation Loss 0.1365, Accuracy 87.11%, Length Loss 0.1797, Accuracy 97.31%, taken 88 sec\n",
      "Epoch 114, Train Loss 0.0590, Accuracy 94.46%; Length Loss 0.0521, Accuracy 98.33% | Validation Loss 0.1383, Accuracy 86.55%, Length Loss 0.2164, Accuracy 96.12%, taken 88 sec\n",
      "Epoch 115, Train Loss 0.0591, Accuracy 94.41%; Length Loss 0.0534, Accuracy 98.34% | Validation Loss 0.1321, Accuracy 87.52%, Length Loss 0.1884, Accuracy 96.99%, taken 88 sec\n",
      "Epoch 116, Train Loss 0.0587, Accuracy 94.47%; Length Loss 0.0515, Accuracy 98.35% | Validation Loss 0.1364, Accuracy 87.51%, Length Loss 0.1974, Accuracy 97.09%, taken 88 sec\n",
      "Epoch 117, Train Loss 0.0598, Accuracy 94.44%; Length Loss 0.0501, Accuracy 98.41% | Validation Loss 0.1388, Accuracy 87.31%, Length Loss 0.2005, Accuracy 96.93%, taken 88 sec\n",
      "Epoch 118, Train Loss 0.0607, Accuracy 94.16%; Length Loss 0.0575, Accuracy 98.17% | Validation Loss 0.1391, Accuracy 87.44%, Length Loss 0.1774, Accuracy 97.36%, taken 88 sec\n",
      "Epoch 119, Train Loss 0.0560, Accuracy 94.79%; Length Loss 0.0496, Accuracy 98.48% | Validation Loss 0.1352, Accuracy 87.61%, Length Loss 0.1703, Accuracy 97.41%, taken 87 sec\n",
      "Epoch 120, Train Loss 0.0562, Accuracy 94.78%; Length Loss 0.0479, Accuracy 98.53% | Validation Loss 0.1422, Accuracy 86.31%, Length Loss 0.1887, Accuracy 96.93%, taken 88 sec\n"
     ]
    }
   ],
   "source": [
    "while epoch_step2 < EPOCHS_STEP2:\n",
    "    step2(epoch_step2)\n",
    "    epoch_step2 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test(glyph):\n",
    "    features = encoder(glyph, training=False)\n",
    "    total_dups = tf.nn.softmax(dup_decoder(features), axis=-1)\n",
    "    dups_dict = tf.math.argmax(total_dups, axis=-1)\n",
    "    max_dup = tf.math.reduce_max(dups_dict) + 1\n",
    "    \n",
    "    results = tf.zeros((glyph.shape[0], max_dup, MAX_LEN + 1), dtype='int64')\n",
    "    probs = tf.zeros((glyph.shape[0], max_dup), dtype='float32')\n",
    "    identity_matrix = tf.convert_to_tensor(np.identity(MAX_DUP), dtype='int64')\n",
    "    \n",
    "    for i in range(max_dup):\n",
    "        curr_dups = tf.math.minimum(tf.math.argmax(total_dups, axis=-1), i)\n",
    "        curr_dups = tf.nn.embedding_lookup(identity_matrix, curr_dups)\n",
    "        length = tf.nn.softmax(length_decoder(features, total_dups, curr_dups), axis=-1)\n",
    "        test_result, prob = predict(features, MAX_LEN, length, total_dups, curr_dups)\n",
    "        results = tf.concat([results[:, :i, :], tf.expand_dims(test_result, axis=1), tf.zeros((glyph.shape[0], max_dup - i - 1, MAX_LEN + 1), dtype='int64')], axis=1)\n",
    "        probs = tf.concat([probs[:, :i], tf.expand_dims(prob, axis=1), tf.zeros((glyph.shape[0], max_dup - i - 1), dtype='float32')], axis=1)\n",
    "    return results, probs, dups_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(word):\n",
    "    test_input = []\n",
    "    for char in word:\n",
    "        glyph = glyphbook.draw(char)\n",
    "        if glyph is not None:\n",
    "            test_input.append(glyph)\n",
    "        else:\n",
    "            raise ValueError('Character {} unsupported.'.format(char))\n",
    "    test_input = np.expand_dims(test_input, -1)\n",
    "    \n",
    "    def decode(indexes):\n",
    "        code = ''\n",
    "        for i in indexes:\n",
    "            if i <= 0:\n",
    "                continue\n",
    "            elif i >= 27:\n",
    "                break\n",
    "            else:\n",
    "                code += chr(i + 96)\n",
    "        return code\n",
    "    \n",
    "    results, probs, dups_dict = test(test_input)\n",
    "    results = results.numpy()\n",
    "    probs = probs.numpy()\n",
    "    dups_dict = dups_dict.numpy()\n",
    "    \n",
    "    final_result = []\n",
    "    for i in range(results.shape[0]):\n",
    "        final_result.append([])\n",
    "        for j in range(results.shape[1]):\n",
    "            if j <= dups_dict[i]:\n",
    "                final_result[-1].append([decode(results[i, j, :]), probs[i, j]])\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['omm', 0.9984736]],\n",
       " [['tghi', 0.9863381]],\n",
       " [['iftwt', 0.99798155], ['mftwt', 0.97513884]],\n",
       " [['ora', 0.9994796]],\n",
       " [['oymr', 0.99512887]],\n",
       " [['eabt', 0.99895555]],\n",
       " [['iav', 0.9820343]],\n",
       " [['tcp', 0.99727935]],\n",
       " [['ooro', 0.997217]],\n",
       " [['yryrv', 0.99954456]]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('仁義禮智信温良恭儉讓')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
