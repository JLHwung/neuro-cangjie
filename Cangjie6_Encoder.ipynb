{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from fontTools.ttLib import TTFont\n",
    "from macrotoolchain import Data, Graph, plot \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glyph(object):\n",
    "    # transform character to bitmap\n",
    "    def __init__(self, fonts, size=64):\n",
    "        # load fonts, size. We will use 2 fonts for all CJK characters, so keep 2 codepoint books.\n",
    "        self.codepoints = [set()] * len(fonts)\n",
    "        self.size = int(size * 0.8)\n",
    "        self.size_img = size\n",
    "        self.pad = (size - self.size) // 2\n",
    "        self.fonts = [ImageFont.truetype(f, self.size) for f in fonts]\n",
    "        # use a cache to reduce computation if duplicated characters encountered.\n",
    "        self.cache = {}\n",
    "        for cp, font in zip(self.codepoints, fonts):\n",
    "            font = TTFont(font)\n",
    "            # store codepoints in font cmap into self.codepoints\n",
    "            for cmap in font['cmap'].tables:\n",
    "                if not cmap.isUnicode():\n",
    "                    continue\n",
    "                for k in cmap.cmap:\n",
    "                    cp.add(k)\n",
    "    \n",
    "    def draw(self, ch):\n",
    "        if ch in self.cache:\n",
    "            return self.cache[ch]\n",
    "        # search among fonts, use the first found\n",
    "        exist = False\n",
    "        for i in range(len(self.codepoints)):\n",
    "            if ord(ch) in self.codepoints[i]:\n",
    "                font = self.fonts[i]\n",
    "                exist = True\n",
    "                break\n",
    "        if not exist:\n",
    "            return None\n",
    "\n",
    "        img = Image.new('L', (self.size_img, self.size_img), 0)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        (width, baseline), (offset_x, offset_y) = font.font.getsize(ch)\n",
    "        draw.text((self.pad - offset_x, self.pad - offset_y + 4), ch, font=font, fill=255, stroke_fill=255) \n",
    "        img_array = np.array(img.getdata(), dtype='float32').reshape((self.size_img, self.size_img)) / 255\n",
    "        self.cache[ch] = img_array\n",
    "\n",
    "        return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphbook = Glyph(['data/fonts/HanaMinA.otf', 'data/fonts/HanaMinB.otf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chart = pd.read_csv('data/cangjie6.txt', delimiter='\\t', header=None, names=['Char', 'Code'], \n",
    "                        keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chart(chart):\n",
    "    glyphs = []\n",
    "    codes = []\n",
    "    for char, code in chart.values:\n",
    "        glyph = glyphbook.draw(char)\n",
    "        if glyph is not None:\n",
    "            glyphs.append(glyph)\n",
    "            codes.append(code)\n",
    "    return np.expand_dims(np.array(glyphs), -1), np.array(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(code_table):\n",
    "    # Cangjie code consists only of a-z, with maximum length of 5, minimum of 1\n",
    "    # start with 0, a-z are 1-26, end and padding are 27\n",
    "    tokens = np.expand_dims(np.zeros(code_table.shape, dtype='int64'), -1)\n",
    "    code_index = list(map(lambda x: list(map(lambda y: ord(y) - 96, list(x))) + [27] * (5-len(x)), code_table))\n",
    "    tokens = np.append(tokens, np.array(code_index), axis=-1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphs, codes = preprocess_chart(code_chart)\n",
    "del code_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_glyphs, validation_glyphs, train_tokens, validation_tokens = train_test_split(\n",
    "    glyphs, tokenizer(codes), test_size=0.1)\n",
    "del glyphs, codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # This is essentially a CNN layer, \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.cnn1 = tf.keras.layers.Convolution2D(8, (5, 5), input_shape=(64, 64, 1))\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.cnn2 = tf.keras.layers.Convolution2D(32, (5, 5))\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.cnn3 = tf.keras.layers.Convolution2D(128, (5, 5))\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape after cnn1 == (batch_size, 60, 60, 8)\n",
    "        x = self.cnn1(x)\n",
    "        # x shape after pool1 == (batch_size, 30, 30, 8)\n",
    "        x = self.pool1(x)\n",
    "        # x shape after cnn2 == (batch_size, 26, 26, 64)\n",
    "        x = self.cnn2(x)\n",
    "        # x shape after pool2 == (batch_size, 13, 13, 64)\n",
    "        x = self.pool2(x)\n",
    "        # x shape after cnn3 == (batch_size, 9, 9, 256)\n",
    "        x = self.cnn3(x)\n",
    "        # reshape from (batch_size, 9, 9, 256) to (batch_size, 81, 256)\n",
    "        x = tf.reshape(x, [x.shape[0], -1, x.shape[-1]])\n",
    "        # x shape after fc == (batch_size, 81, embedding_dim)\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bahdanau_Attention(tf.keras.Model):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Bahdanau_Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.W2 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_Encoder output) shape == (batch_size, 81, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 81, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 81, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, embedding_dim)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, hidden_size, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = Bahdanau_Attention(hidden_size)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape before is (batch_size, 1) since it is passed through one by one at a time\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # context_vector shape is (batch_size, embedding_dim)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output shape is (batch_size, 1, hidden_size)\n",
    "        # state is new hidden used in next step\n",
    "        output, state = self.gru(x)\n",
    "        # x shape == (batch_size, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        # x shape == (batch_size, vocab)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        # generate new hidden layer with different batch size\n",
    "        return tf.zeros((batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim=128)\n",
    "decoder = RNN_Decoder(embedding_dim=128, hidden_size=128, vocab_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracy = tf.math.reduce_all(pred == real, 1)\n",
    "    return tf.math.reduce_mean(tf.cast(accuracy, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def predict(glyph_tensor, max_length):\n",
    "    features = encoder(glyph_tensor)\n",
    "    # start with 0\n",
    "    dec_input = tf.convert_to_tensor([[0]]*glyph_tensor.shape[0], dtype='int64')\n",
    "    hidden = decoder.reset_state(batch_size=glyph_tensor.shape[0])\n",
    "    # iterate predictions, no teacher forcing here\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(tf.expand_dims(dec_input[:, i], 1), features, hidden)\n",
    "        # we need deterministic result\n",
    "        predicted_id = tf.math.argmax(predictions, axis=-1)\n",
    "        dec_input = tf.concat([dec_input, tf.expand_dims(predicted_id, -1)], axis=1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(glyph_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the codes are not related from glyph to glyph\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims(target[:, 0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(glyph_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    # calculate accuracy based on the code's whole string\n",
    "    predictions = predict(glyph_tensor, target.shape[1] - 1)\n",
    "    accuracy = accuracy_function(predictions, target)\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss / int(target.shape[1] - 1), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(glyph_tensor, target):\n",
    "    val_loss = 0\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    features = encoder(glyph_tensor)\n",
    "    dec_input = tf.expand_dims(target[:, 0], 1)\n",
    "    \n",
    "    for i in range(1, target.shape[1]):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "        val_loss += loss_function(target[:, i], predictions)\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(target[:, i], 1)\n",
    "    \n",
    "    # calculate accuracy based on the code's whole string\n",
    "    val_validations = predict(glyph_tensor, target.shape[1] - 1)\n",
    "    val_accuracy = accuracy_function(val_validations, target)\n",
    "    \n",
    "    return val_loss / int(target.shape[1]-1), val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_glyphs, train_tokens))\n",
    "dataset = dataset.shuffle(2048).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a checkpoint to store weights\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, progression 100.0%, time elapsed 343.30 sec\r"
     ]
    }
   ],
   "source": [
    "history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "EPOCHS = 200\n",
    "num_steps = len(train_glyphs) // BATCH_SIZE\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (glyph_tensor, target)) in enumerate(dataset):\n",
    "        t_loss, accuracy = train_step(glyph_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        print(f'Epoch {epoch + 1}, progression {batch / num_steps:.1%}, time elapsed {time.time() - start:.2f} sec', end='\\r')\n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "    \n",
    "    val_loss, val_accuracy = validation_step(validation_glyphs, validation_tokens)\n",
    "    # storing the epoch end loss value to plot later\n",
    "    history['loss'].append(total_loss / num_steps)\n",
    "    history['accuracy'].append(accuracy)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    print (f'Epoch {epoch + 1}, Loss {total_loss/num_steps:.6f}, Accuracy {accuracy:.2%}; Validation Loss {val_loss:.6f}, Validation Accuracy {val_accuracy:.2%}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyData = Data(np.arange(1, len(history['accuracy'])+1), history['accuracy'], 'Training Accuracy')\n",
    "valAccuracyData = Data(np.arange(1, len(history['val_accuracy'])+1), history['val_accuracy'],\n",
    "                       'Validation Accuracy')\n",
    "plot(Graph(accuracyData), Graph(valAccuracyData), ytickformat='.2%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(word):\n",
    "    test_input = []\n",
    "    for char in word:\n",
    "        glyph = glyphbook.draw(char)\n",
    "        if glyph is not None:\n",
    "            test_input.append(glyph)\n",
    "        else:\n",
    "            raise ValueError(f'Character {char} unsupported.')\n",
    "    test_input = tf.expand_dims(test_input, -1)\n",
    "    test_result = predict(test_input, 5)\n",
    "\n",
    "    def decode(indexes):\n",
    "        code = ''\n",
    "        for i in indexes:\n",
    "            if i <= 0:\n",
    "                continue\n",
    "            elif i >= 27:\n",
    "                break\n",
    "            else:\n",
    "                code += chr(i + 96)\n",
    "        return code\n",
    "\n",
    "    return np.apply_along_axis(decode, 1, test_result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('劉運操')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正確倉頡碼：hhcln, ybjj, qrrd"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
